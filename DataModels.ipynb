{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "70c033a1-461a-4f8b-a2f1-58560ad848b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Customization imports\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import time\n",
    "\n",
    "# Export dataFrame's as images\n",
    "import dataframe_image as dfi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a9f871a2-c164-41df-835f-fe553ad05a1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.compose import make_column_selector, make_column_transformer\n",
    "from sklearn.preprocessing import OneHotEncoder, PolynomialFeatures, OrdinalEncoder\n",
    "from sklearn.preprocessing import StandardScaler, RobustScaler, MinMaxScaler\n",
    "\n",
    "from sklearn.linear_model import LinearRegression, LogisticRegression, Ridge, Lasso\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score\n",
    "from sklearn.metrics import ConfusionMatrixDisplay, RocCurveDisplay\n",
    "from sklearn.metrics import confusion_matrix, roc_curve, auc\n",
    "from sklearn.metrics import auc as auc_temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0314a970-203d-46e8-a6ad-3924aeec2d70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import my utility methods for this project\n",
    "import utils_practical_2 as my_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "91a2e952-e0fa-4189-b54e-0c8c922293f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure logging\n",
    "import logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(levelname)s: %(message)s')\n",
    "\n",
    "# logging.getLogger().setLevel(logging.DEBUG)\n",
    "# logging.getLogger().setLevel(logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1f850e11-f596-432d-b59d-dba838f9d8b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Global format rules\n",
    "\n",
    "# Clean up axes labels\n",
    "axes_fonts = {'fontweight': 'bold'}\n",
    "title_fonts = {'fontweight': 'bold', 'fontsize': 14}\n",
    "\n",
    "# Seaborn over-rides\n",
    "sns.set_theme(style='darkgrid')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce56df00-e340-42e6-855d-0df5e3e265af",
   "metadata": {},
   "source": [
    "# On to Modeling ...\n",
    "\n",
    "**This notebook picks up from the Data Investigation (see ```DataInvestigation.ipynb```)**\n",
    "\n",
    "[Local file](DataInvestigation.ipynb)\n",
    "[Github](https://github.com/fazeelgm/UCB_ML_AI_PracticalApp_II/blob/main/DataInvestigation.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a42a564e-b12a-439c-a15d-342c0054f9ed",
   "metadata": {},
   "source": [
    "## Data Cleanup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ccd992b-4eb6-494d-ba4d-c575bef1297c",
   "metadata": {},
   "source": [
    "The data cleansing results are as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5bf7ae0c-5f0b-4bf6-8211-ff40a719cf78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# vehicles_raw, vehicles_cleansed = my_utils.get_cleansed_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bfee5084-49bb-40b1-aa09-a29e63d87c4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# vehicles_cleansed.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82496735-dab0-4dae-b489-06e0f21cbeec",
   "metadata": {},
   "source": [
    "# On to Modeling ...\n",
    "\n",
    "While investigating the different features of our dataset during the data investigation, I learned two things:\n",
    "\n",
    "1. The data is very noisy with extreme outliers - I removed null data and outliers as much as possible\n",
    "   * 66,180 samples were dropped, preserving 84.50% of the original data\n",
    "3. To aid in this effort, I researched the used car marketplace to get some idea of pricing, important features that drive price and potential inventory segments"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0e233e8-2531-4fef-af0b-6d1d9586e512",
   "metadata": {},
   "source": [
    "## Initial Hypothesis\n",
    "\n",
    "Based on Price inspection, a potential hypothesis arose that the used car inventory is _segmented_ based on the following price bands:\n",
    "\n",
    "![](images/candidate-price-segments.png)\n",
    "\n",
    "In addition, market research suggested that typical used cars can be categoriezed into price ranges like Budget, Mid, Luxury, etc. that will be based on feature groups. I looked at ```<price, year, condition, odometer>``` combinations and saw that there was clustering behavior as shown by the scatter plots below:\n",
    "\n",
    "![](images/scatter-price-odo-condition-budget.png)\n",
    "![](images/scatter-price-odo-year-entry.png)\n",
    "\n",
    "So, I will now use clustering techniques to see if we can observe natural clustering of features in our sample population."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c54a9e63-0dae-45a9-a9f8-f2875be36cd1",
   "metadata": {},
   "source": [
    "# Final Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a074eed-a8f0-47dc-8dfa-14c96314a85d",
   "metadata": {},
   "source": [
    "## Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e9ab1651-5568-4403-8d34-c106732d5680",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading data/vehicles.csv ... Done: (426880, 18)\n",
      "\n",
      "Cleansing price column ... \n",
      "... Removing price outliers using ModZ method ... \n",
      "... ModZ: 9450.0, med: 13950.0, const: 0.6745\n",
      "... Time: 0.12219381332397461\n",
      "... Removed 5,790 outliers\n",
      "... Removing cars with price = 0 ...  Removed 32,895 rows\n",
      "Done: (421090, 19) -> (388195, 19)\n",
      "\n",
      "DropNA from columns: \n",
      "... year: 1,029 rows (0.27% of total): 388,195 -> 387,166\n",
      "... manufacturer: 16,609 rows (4.28% of total): 388,195 -> 371,586\n",
      "... fuel: 19,173 rows (4.94% of total): 388,195 -> 369,022\n",
      "... title_status: 26,730 rows (6.89% of total): 388,195 -> 361,465\n",
      "... odometer: 28,960 rows (7.46% of total): 388,195 -> 359,235\n",
      "... transmission: 30,742 rows (7.92% of total): 388,195 -> 357,453\n",
      "Done: (388195, 19) -> (360700, 19)\n",
      "\n",
      "Dropping columns: ['mod_zscore', 'id', 'model']\n",
      "... mod_zscore\n",
      "... id\n",
      "... model\n",
      "Done: (360700, 19) -> (360700, 16)\n",
      "\n",
      "Data Transformations:\n",
      "... year float -> int: Done\n",
      "... odometer float -> int: Done\n",
      "\n",
      "Category Transformations:\n",
      "... Converting column \"condition\" -> Category: Done\n",
      "... Converting column \"manufacturer\" -> Category: Done\n",
      "... Converting column \"cylinders\" -> Category: Done\n",
      "... Converting column \"fuel\" -> Category: Done\n",
      "... Converting column \"title_status\" -> Category: Done\n",
      "... Converting column \"state\" -> Category: Done\n",
      "... Converting column \"transmission\" -> Category: Done\n",
      "... Converting column \"drive\" -> Category: Done\n",
      "... Converting column \"size\" -> Category: Done\n",
      "... Converting column \"type\" -> Category: Done\n",
      "... Converting column \"paint_color\" -> Category: Done\n",
      "\n",
      "Returned Raw(426,880x19) and Cleansed(360,700x16) data\n",
      "Dataset reduced by 66,180 rows (preserved 84.50% of total)\n"
     ]
    }
   ],
   "source": [
    "vehicles_raw, vehicles_cleansed = my_utils.get_cleansed_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "80a0eb11-fb70-44ec-8cbe-dbc291a4b28c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 360700 entries, 27 to 426879\n",
      "Data columns (total 16 columns):\n",
      " #   Column        Non-Null Count   Dtype   \n",
      "---  ------        --------------   -----   \n",
      " 0   region        360700 non-null  object  \n",
      " 1   price         360700 non-null  int64   \n",
      " 2   year          360700 non-null  int64   \n",
      " 3   manufacturer  360700 non-null  category\n",
      " 4   condition     223668 non-null  category\n",
      " 5   cylinders     213522 non-null  category\n",
      " 6   fuel          360700 non-null  category\n",
      " 7   odometer      360700 non-null  int64   \n",
      " 8   title_status  360700 non-null  category\n",
      " 9   transmission  360700 non-null  category\n",
      " 10  VIN           219818 non-null  object  \n",
      " 11  drive         252582 non-null  category\n",
      " 12  size          104338 non-null  category\n",
      " 13  type          282300 non-null  category\n",
      " 14  paint_color   257706 non-null  category\n",
      " 15  state         360700 non-null  category\n",
      "dtypes: category(11), int64(3), object(2)\n",
      "memory usage: 20.3+ MB\n"
     ]
    }
   ],
   "source": [
    "vehicles_cleansed.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d2d79a32-2e61-47b6-a236-479b121043a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['region', 'price', 'year', 'manufacturer', 'condition', 'cylinders',\n",
       "       'fuel', 'odometer', 'title_status', 'transmission', 'VIN', 'drive',\n",
       "       'size', 'type', 'paint_color', 'state'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vehicles_cleansed.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f2ee0ea1-5830-4599-b58f-81138e76b0bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a copy of the clean data\n",
    "data = vehicles_cleansed.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "aecbd077-08d4-40a9-9232-15bae5d5f945",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove unwanted columns\n",
    "drop_cols = ['region', 'manufacturer', 'VIN', 'paint_color', 'state', 'size']\n",
    "data.drop(columns=drop_cols, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c402754d-825c-486e-ac92-4cd54c5473a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 360700 entries, 27 to 426879\n",
      "Data columns (total 10 columns):\n",
      " #   Column        Non-Null Count   Dtype   \n",
      "---  ------        --------------   -----   \n",
      " 0   price         360700 non-null  int64   \n",
      " 1   year          360700 non-null  int64   \n",
      " 2   condition     223668 non-null  category\n",
      " 3   cylinders     213522 non-null  category\n",
      " 4   fuel          360700 non-null  category\n",
      " 5   odometer      360700 non-null  int64   \n",
      " 6   title_status  360700 non-null  category\n",
      " 7   transmission  360700 non-null  category\n",
      " 8   drive         252582 non-null  category\n",
      " 9   type          282300 non-null  category\n",
      "dtypes: category(7), int64(3)\n",
      "memory usage: 13.4 MB\n"
     ]
    }
   ],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "17caea75-fbce-453a-9cd4-e27a76e018fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data['size'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1c842463-3057-48eb-b01f-88ae38930738",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data.shape[0] - data['size'].value_counts().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "86554ad8-0390-4246-add6-c036b594d369",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data[data['size'].isna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "4bf2c88c-2d19-411a-9979-9fab5d1523e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data.query('size == \"\"')['size'].value_counts().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c549e347-221b-4476-a120-f89cefc2338b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dropping nulls: (360700, 10) ->  (122682, 10)\n"
     ]
    }
   ],
   "source": [
    "# remove nulls before splitting data\n",
    "\n",
    "dropna_cols = ['condition', 'cylinders', 'drive', 'type']\n",
    "\n",
    "print('Dropping nulls: {} -> '.format(data.shape), end='')\n",
    "# data.dropna(subset=['condition', 'cylinders', 'drive', 'size', 'type'], axis='index', inplace=True)\n",
    "data.dropna(subset=dropna_cols, axis='index', inplace=True)\n",
    "print(' {}'.format(data.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ae860574-3cff-4b83-9a4c-612ef3a10eb0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['price', 'year', 'condition', 'cylinders', 'fuel', 'odometer',\n",
       "       'title_status', 'transmission', 'drive', 'type'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d77117c-469c-4bf8-895b-99daeb18db79",
   "metadata": {},
   "source": [
    "## Feature Transformation for Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5e3a2d7-bc51-4f7e-835a-a4fb8f249080",
   "metadata": {},
   "source": [
    "### Create Train/Test splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "a2376fcd-789f-44ec-abe4-dc556a74d862",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data.drop('price', axis='columns')\n",
    "y = data['price']\n",
    "\n",
    "# OneHot Encode the features\n",
    "X = pd.get_dummies(X, drop_first=True)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "be3c897e-691a-4954-b786-88ab9e08a3de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 122682 entries, 31 to 426878\n",
      "Data columns (total 39 columns):\n",
      " #   Column                   Non-Null Count   Dtype\n",
      "---  ------                   --------------   -----\n",
      " 0   year                     122682 non-null  int64\n",
      " 1   odometer                 122682 non-null  int64\n",
      " 2   condition_fair           122682 non-null  bool \n",
      " 3   condition_good           122682 non-null  bool \n",
      " 4   condition_like new       122682 non-null  bool \n",
      " 5   condition_new            122682 non-null  bool \n",
      " 6   condition_salvage        122682 non-null  bool \n",
      " 7   cylinders_12 cylinders   122682 non-null  bool \n",
      " 8   cylinders_3 cylinders    122682 non-null  bool \n",
      " 9   cylinders_4 cylinders    122682 non-null  bool \n",
      " 10  cylinders_5 cylinders    122682 non-null  bool \n",
      " 11  cylinders_6 cylinders    122682 non-null  bool \n",
      " 12  cylinders_8 cylinders    122682 non-null  bool \n",
      " 13  cylinders_other          122682 non-null  bool \n",
      " 14  fuel_electric            122682 non-null  bool \n",
      " 15  fuel_gas                 122682 non-null  bool \n",
      " 16  fuel_hybrid              122682 non-null  bool \n",
      " 17  fuel_other               122682 non-null  bool \n",
      " 18  title_status_lien        122682 non-null  bool \n",
      " 19  title_status_missing     122682 non-null  bool \n",
      " 20  title_status_parts only  122682 non-null  bool \n",
      " 21  title_status_rebuilt     122682 non-null  bool \n",
      " 22  title_status_salvage     122682 non-null  bool \n",
      " 23  transmission_manual      122682 non-null  bool \n",
      " 24  transmission_other       122682 non-null  bool \n",
      " 25  drive_fwd                122682 non-null  bool \n",
      " 26  drive_rwd                122682 non-null  bool \n",
      " 27  type_bus                 122682 non-null  bool \n",
      " 28  type_convertible         122682 non-null  bool \n",
      " 29  type_coupe               122682 non-null  bool \n",
      " 30  type_hatchback           122682 non-null  bool \n",
      " 31  type_mini-van            122682 non-null  bool \n",
      " 32  type_offroad             122682 non-null  bool \n",
      " 33  type_other               122682 non-null  bool \n",
      " 34  type_pickup              122682 non-null  bool \n",
      " 35  type_sedan               122682 non-null  bool \n",
      " 36  type_truck               122682 non-null  bool \n",
      " 37  type_van                 122682 non-null  bool \n",
      " 38  type_wagon               122682 non-null  bool \n",
      "dtypes: bool(37), int64(2)\n",
      "memory usage: 7.1 MB\n"
     ]
    }
   ],
   "source": [
    "X.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c13a99ac-6550-4aad-a8e4-5f488f99df26",
   "metadata": {},
   "source": [
    "### Feature Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "92063183-92e5-42d3-862f-a95852ba03b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale the data - we'll use StandardScaler for the baseline models\n",
    "logging.debug('Scaling data')\n",
    "scaler = StandardScaler()\n",
    "\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.fit_transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7770b4e-e9dd-42c2-9556-c440ab883821",
   "metadata": {},
   "source": [
    "## Baseline Regression Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "a59b2fd5-fee4-4d84-a424-8d7b7b0cfb1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "models = {\n",
    "    'LinearRegression' : LinearRegression(),\n",
    "    'Ridge' : Ridge(alpha=1.0),\n",
    "    'Lasso' : Lasso(alpha=0.1)\n",
    "}\n",
    "\n",
    "# save results for tabulation\n",
    "results_baseline = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "d3d4abf1-64a3-4f7f-8884-6963da4bb72d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate models\n",
    "def get_model_metrics_as_results(name, clf, scaler, X_test, y_test):\n",
    "    \"\"\"\n",
    "    Build standardized results row given the predictions and y_test values\n",
    "\n",
    "    :param name: Model name for labeling the row in the table\n",
    "    :param clf: Fitted classifier to get metrics\n",
    "    :param scaler: Fitted scaler used for transformation\n",
    "    :param X_test: Test data used to fit the classifier\n",
    "    :param y_test: Evaluation data to be used for the metrics\n",
    "    :return: Returns single row of results summary table containing:\n",
    "    \n",
    "        [model_name, MAE, MSE, RMSE, R2_Score, y-intercept]\n",
    "    \"\"\"\n",
    "    \n",
    "    logging.debug(f'Working on {name}')\n",
    "\n",
    "    # Get predictions\n",
    "    y_preds = clf.predict(X_test)\n",
    "\n",
    "    # get metrics\n",
    "    mae = mean_absolute_error(y_preds, y_test)\n",
    "    mse = mean_squared_error(y_preds, y_test)\n",
    "    rmse = np.sqrt(mse)\n",
    "    r2 = r2_score(y_test, y_preds)\n",
    "    score = clf.score(X_test, y_test)\n",
    "    y_intercept = np.abs(clf.intercept_)\n",
    "    sname = scaler.__class__.__name__\n",
    "    \n",
    "    logging.debug(f'... {name}: Scaler: {sname} MAE: {mae:,.4f}, MSE: {mse:,.4f}, RMSE: {rmse:,.4f}, R2: {r2:,.4f}, Score: {score:,.4f}, y-int: {y_intercept:,.4f}')\n",
    "\n",
    "    return [name, sname, mae, mse, rmse, score, y_intercept]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "0718d7ce-19fa-4e61-9cbb-d7f7d6fcf151",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working on LinearRegression ... Done with LinearRegression (0.1270 sec)\n",
      "Working on Ridge ... Done with Ridge (0.0382 sec)\n",
      "Working on Lasso ... Done with Lasso (2.0984 sec)\n"
     ]
    }
   ],
   "source": [
    "# Enable debugging output\n",
    "# logging.getLogger().setLevel(logging.DEBUG)\n",
    "\n",
    "results_baseline = []\n",
    "\n",
    "# iterate over the models and build results DF\n",
    "for model in models:\n",
    "\n",
    "    start_time = time.time()\n",
    "    print(f'Working on {model} ... ', end='')\n",
    "\n",
    "    # Fit the model\n",
    "    models[model].fit(X_train_scaled, y_train)\n",
    "    \n",
    "    # Append model metrics to results\n",
    "    results_baseline.append(get_model_metrics_as_results(model, models[model], scaler, X_test_scaled, y_test))\n",
    "\n",
    "    print(f'Done with {model} ({time.time() - start_time:,.4f} sec)')\n",
    "\n",
    "# Disable debugging\n",
    "logging.getLogger().setLevel(logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "98e2bb0c-e5d6-4554-9e65-64a6b0bbf35e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RESULTS: Baseline metrics without any optimization\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Scaler</th>\n",
       "      <th>MAE</th>\n",
       "      <th>MSE</th>\n",
       "      <th>RMSE</th>\n",
       "      <th>R2 Score</th>\n",
       "      <th>Base Price</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Model</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>LinearRegression</th>\n",
       "      <td>StandardScaler</td>\n",
       "      <td>5,959.6608</td>\n",
       "      <td>71,289,759.7592</td>\n",
       "      <td>8,443.3263</td>\n",
       "      <td>0.5289</td>\n",
       "      <td>15,707.9495</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Ridge</th>\n",
       "      <td>StandardScaler</td>\n",
       "      <td>5,959.6674</td>\n",
       "      <td>71,289,772.2857</td>\n",
       "      <td>8,443.3271</td>\n",
       "      <td>0.5289</td>\n",
       "      <td>15,707.9495</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Lasso</th>\n",
       "      <td>StandardScaler</td>\n",
       "      <td>5,959.6765</td>\n",
       "      <td>71,289,852.2068</td>\n",
       "      <td>8,443.3318</td>\n",
       "      <td>0.5289</td>\n",
       "      <td>15,707.9495</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          Scaler         MAE              MSE        RMSE  \\\n",
       "Model                                                                       \n",
       "LinearRegression  StandardScaler  5,959.6608  71,289,759.7592  8,443.3263   \n",
       "Ridge             StandardScaler  5,959.6674  71,289,772.2857  8,443.3271   \n",
       "Lasso             StandardScaler  5,959.6765  71,289,852.2068  8,443.3318   \n",
       "\n",
       "                 R2 Score   Base Price  \n",
       "Model                                   \n",
       "LinearRegression   0.5289  15,707.9495  \n",
       "Ridge              0.5289  15,707.9495  \n",
       "Lasso              0.5289  15,707.9495  "
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Generate results table\n",
    "results_baseline_df = pd.DataFrame(results_baseline, \n",
    "                                   columns=['Model', 'Scaler', 'MAE', 'MSE', 'RMSE', 'R2 Score', 'Base Price']\n",
    "                                  ).set_index('Model')\n",
    "\n",
    "\n",
    "# Export results for README\n",
    "results_baseline_df_styled = my_utils.df_style_floats(results_baseline_df)\n",
    "dfi.export(results_baseline_df_styled, 'images/results_baseline_table.png')\n",
    "\n",
    "print('RESULTS: Baseline metrics without any optimization\\n')\n",
    "results_baseline_df_styled"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f8b482d-b680-418e-9ab8-9904d1dbe783",
   "metadata": {},
   "source": [
    "All the models performed very closely without any clear winners. Given the target variable measurement unit, price in dollars per car:\n",
    "\n",
    "* All the models showed roughly $8,287 (RMSE) average difference between the predicted and actual prices from the test dataset that was not used for the training of the model\n",
    "* The R2 Score shows thay they each captured 42.77% of the variance in the price\n",
    "\n",
    "We will now optimize the models to see if we can improve the results and pick a clear winner."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adebdccf-fb94-404c-951e-5547112d66b8",
   "metadata": {},
   "source": [
    "## Model Tuning: Hyperparameter Tuning and Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "929bacba-0403-4875-8331-2b28dff64c87",
   "metadata": {},
   "outputs": [],
   "source": [
    "models = {\n",
    "    'Ridge': Ridge(),\n",
    "    'Lasso': Lasso(),\n",
    "    'LinearRegression': LinearRegression()\n",
    "}\n",
    "\n",
    "cv_param_grid = {\n",
    "    'Ridge': {'alpha': [0.1, 1.0, 10.0, 100.0]},\n",
    "    'Lasso': {'alpha': [0.01, 0.1, 1.0, 10.0]}    \n",
    "}\n",
    "\n",
    "scalers = {\n",
    "    StandardScaler(),\n",
    "    RobustScaler(),    # default: quantile_range=(25, 75)\n",
    "    MinMaxScaler()\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "d343d7b2-6ffa-448d-be6a-ea08bcb5516c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Toggle debugging output\n",
    "# logging.getLogger().setLevel(logging.DEBUG)\n",
    "# logging.getLogger().setLevel(logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09c47a07-353d-4f88-b5da-60e7069d954a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working on StandardScaler() ... \n",
      "\tWorking on Ridge ... Done with Ridge (1.1645 sec)\n",
      "\tWorking on Lasso ... Done with Lasso (32.6861 sec)\n",
      "\tWorking on LinearRegression ... Done with LinearRegression (0.8450 sec)\n",
      "Done with StandardScaler()\n",
      "Working on MinMaxScaler() ... \n",
      "\tWorking on Ridge ... Done with Ridge (0.9625 sec)\n",
      "\tWorking on Lasso ... Done with Lasso (25.1579 sec)\n",
      "\tWorking on LinearRegression ... Done with LinearRegression (0.8050 sec)\n",
      "Done with MinMaxScaler()\n",
      "Working on RobustScaler() ... \n",
      "\tWorking on Ridge ... "
     ]
    }
   ],
   "source": [
    "#\n",
    "# Set loops to cross-validate over model / scaler combinations using GridSearchCV\n",
    "#\n",
    "# save results for tabulation\n",
    "results_tuned = []\n",
    "best_models = {}\n",
    "best_params = {}\n",
    "# cv_scores = {}\n",
    "cv_rmses = {}\n",
    "\n",
    "# Cross-validation count\n",
    "cv=5\n",
    "\n",
    "for scaler in scalers:\n",
    "\n",
    "    print(f'Working on {scaler} ... ')\n",
    "    logging.debug(f'Working on {scaler}')\n",
    "\n",
    "    # Scale X_train\n",
    "    logging.debug(f'Scaling data using {scaler}')\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_test_scaled = scaler.fit_transform(X_test)\n",
    "\n",
    "    for model in models:\n",
    "\n",
    "        start_time = time.time()\n",
    "\n",
    "        print(f'\\tWorking on {model} ... ', end='')\n",
    "\n",
    "        clf = models[model]\n",
    "\n",
    "        # Handle CV models\n",
    "        if (model in cv_param_grid):\n",
    "            logging.debug('{} : {}, Scaler: {}'.format(model, cv_param_grid[model], scaler))\n",
    "            grid = GridSearchCV(clf, param_grid=cv_param_grid[model], cv=cv, scoring='neg_mean_squared_error')\n",
    "            grid.fit(X_train_scaled, y_train)\n",
    "              \n",
    "            # Save the tuning metrics for this model class\n",
    "            best_models[model] = grid.best_estimator_\n",
    "            best_params[model] = grid.best_params_\n",
    "            logging.debug(f'Best param: {grid.best_params_}')            \n",
    "            cv_rmses[model] = np.sqrt(-grid.best_score_)\n",
    "\n",
    "        # Handle non-CV models\n",
    "        else:\n",
    "            logging.debug('{}, Scaler: {}'.format(model, scaler))\n",
    "            clf.fit(X_train_scaled, y_train)\n",
    "            cv_scores = cross_val_score(clf, X_train_scaled, y_train, cv=cv, scoring='neg_mean_squared_error')\n",
    "\n",
    "            # Save the tuning metrics for this model class\n",
    "            best_models[model] = clf\n",
    "            best_params[model] = ''\n",
    "            cv_rmses[model] = np.sqrt(-np.mean(cv_scores))\n",
    "\n",
    "        y_preds = best_models[model].predict(X_test_scaled)\n",
    "\n",
    "        # Get the predictions for this model instance\n",
    "        score = best_models[model].score(X_test_scaled, y_test)\n",
    "        r2 = r2_score(y_test, y_preds)\n",
    "        logging.debug(f'CV Best RMSE: {cv_rmses[model]}')\n",
    "        logging.debug(f'Score: {score}, r2: {r2}')\n",
    "       \n",
    "        # Append model metrics to results\n",
    "        results_tuned.append(get_model_metrics_as_results(model, best_models[model], scaler, X_test_scaled, y_test))\n",
    "        # results_tuned.append(get_model_metrics_as_results(model, y_preds, y_test, r2))\n",
    "\n",
    "        print(f'Done with {model} ({time.time() - start_time:,.4f} sec)')\n",
    "\n",
    "    print(f'Done with {scaler}')\n",
    "\n",
    "logging.getLogger().setLevel(logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f605bd59-1723-4063-b555-528709cdaa15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate results table\n",
    "results_tuned_df = pd.DataFrame(results_tuned,\n",
    "                                columns=['Model - Tuned', 'Scaler', 'MAE', 'MSE', 'RMSE', 'R2 Score', 'Base Price']\n",
    "                               ).set_index('Model - Tuned')\n",
    "\n",
    "# Add BestParams as a new column\n",
    "results_tuned_df['CV RMSE'] = results_tuned_df.index.map(cv_rmses)\n",
    "results_tuned_df['CV Best Params'] = results_tuned_df.index.map(best_params)\n",
    "\n",
    "# Export results for README\n",
    "results_tuned_df_styled = my_utils.df_style_floats(results_tuned_df)\n",
    "dfi.export(results_tuned_df_styled, 'images/results_tuned_table.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c5ef09b-2b40-4a70-9903-d34322f80741",
   "metadata": {},
   "source": [
    "## Model Assessment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4137b4d-4ac1-4196-a0fd-493263cc8514",
   "metadata": {},
   "source": [
    "### Best Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85ef569b-d193-4624-b642-dbe7b218cf0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('RESULTS: Baseline Model metrics without any optimization\\n')\n",
    "results_baseline_df_styled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "416fb7b5-41d5-4962-abf6-bf0f496bdaba",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('RESULTS: Optimized Model results after hyperparameter tuning and cross-validation\\n')\n",
    "results_tuned_df_styled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "080be12d-d864-4520-b028-4f751fdb2b4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model_idx = results_tuned_df.reset_index()['R2 Score'].idxmax()\n",
    "best_model_name = results_tuned_df.index[best_model_idx]\n",
    "best_model = best_models[best_model_name]\n",
    "y_intercept = np.abs(best_model.intercept_)\n",
    "results_row = results_tuned_df.iloc[best_model_idx]\n",
    "\n",
    "print('Best Model with highest R2 Score:\\n\\t{}{}\\n\\tScaler: {}\\n\\tR2 Score: {:,.2f}%\\n\\tRMSE: ${}\\n\\tBase Price (y-intercept): ${:,.2f}'\n",
    "      .format(best_model_name,\n",
    "              results_row['CV Best Params'],\n",
    "              scaler.__class__.__name__,\n",
    "              float(results_row['R2 Score']) * 100, \n",
    "              results_row['RMSE'],\n",
    "              y_intercept))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75722831-f41c-4739-9428-a965ae512e97",
   "metadata": {},
   "source": [
    "Again, all the models performed very closely without any clear winners. \n",
    "\n",
    "However, the optimized Ridge Model with alpha=10.0 showed the least average difference between predicted and actual prices (\\\\$8,281.94 vs \\\\$8,287 RMSE) and a captured more variance across the features (42.85%  vs 42.77% R2 Score) for the **best performing Ridge model**. \n",
    "\n",
    "In addition, the base price of a car is given by the y-intercept for the model: $22,490.54, with each feature adding or subtracting from this value based on the training features."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03141659-c776-4a34-a56d-1d5b0398d03d",
   "metadata": {},
   "source": [
    "### Visualizing Actual vs Predicted Prices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfdbff29-0581-465c-adc2-5db76699bfe4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot predictions\n",
    "y_preds = best_model.predict(X_test_scaled)\n",
    "\n",
    "# Adjust for the y-intercept\n",
    "y_preds = np.add(y_preds, y_intercept)\n",
    "perfect_pt_1 = [min(y_test), max(y_test)]\n",
    "perfect_pt_2 = [min(y_test), max(y_test)] + y_intercept\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.scatter(x=y_test, y=y_preds, label=f'{best_model_name} Model', alpha=0.5)\n",
    "plt.plot(perfect_pt_1, perfect_pt_2, color='red', linestyle='--', label='Theoretical Perfect Predictions')\n",
    "\n",
    "plt.xlabel('Actual Prices', fontdict=axes_fonts)\n",
    "plt.ylabel('Predicted Prices', fontdict=axes_fonts)\n",
    "plt.title(f'Model Predictions vs Actual Prices for the best performing {best_model_name} Model (y-intercept=${y_intercept:,.2f})', \n",
    "          fontdict=title_fonts)\n",
    "plt.legend()\n",
    "\n",
    "plt.savefig('images/scatter-optimized-preds-v-test.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "442dc4b0-d284-4685-8967-19ff17037b0b",
   "metadata": {},
   "source": [
    "### PredictionErrorDisplay: Residuals Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "472e687a-3adb-4e9b-a893-8105500ba542",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # How to get the list of available Displays for my version of sklearn\n",
    "# from sklearn.utils.discovery import all_displays\n",
    "# displays = all_displays()\n",
    "# displays\n",
    "\n",
    "from sklearn.metrics import PredictionErrorDisplay\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 6))\n",
    "PredictionErrorDisplay.from_estimator(best_model, X_test_scaled, y_test, ax=axes[0], kind=\"actual_vs_predicted\")\n",
    "PredictionErrorDisplay.from_estimator(best_model, X_test_scaled, y_test, ax=axes[1], kind=\"residual_vs_predicted\")\n",
    "\n",
    "plt.suptitle(f'Best Model: Comparing Predictions vs Actual Prices (without y-intercept=${y_intercept:,.2f})')\n",
    "plt.savefig('images/optimized-PredictionErrorDisplay.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13ab8887-176c-4a0a-9a64-4d4964ee3ee4",
   "metadata": {},
   "source": [
    "### Interpreting the Model Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2f7b5df-1d7d-4d95-a909-7c5d6fdbd834",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# build the feature importance DF\n",
    "\n",
    "feature_names = X.columns\n",
    "\n",
    "feature_imp = []\n",
    "for feature, coef in zip(feature_names, best_model.coef_):\n",
    "    if coef != 0:\n",
    "        # print(f'{feature}: {coef}')\n",
    "        feature_imp.append([feature, coef])\n",
    "\n",
    "feature_imp_df = pd.DataFrame(data=feature_imp, columns=['Feature', 'Coefficient'])#.set_index('Feature')\n",
    "feature_imp_df['Importance'] = np.abs(feature_imp_df.Coefficient)\n",
    "\n",
    "# Sort the DF\n",
    "feature_imp_df = feature_imp_df.sort_values(by='Importance', ascending=False)\n",
    "\n",
    "# Add cumulative importance\n",
    "feature_imp_df['Cumulative Importance'] = feature_imp_df['Importance'].cumsum() / feature_imp_df['Importance'].sum()\n",
    "feature_imp_df['Percent Contribution'] = feature_imp_df['Cumulative Importance'] * 100\n",
    "\n",
    "print(f'Impact of each feature to the Base Price: ${y_intercept:,.2f} (y-intercept)')\n",
    "my_utils.df_style_floats(feature_imp_df[['Feature', 'Coefficient', 'Percent Contribution']].set_index('Feature').rename(columns={'Coefficient':'Price Impact'}).head())\n",
    "# feature_imp_df_styled = my_utils.df_style_floats(feature_imp_df)\n",
    "# feature_imp_df_styled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d53d43a8-3dd0-4e35-ade4-31af059ee2bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Top features that explain X% of the variance\n",
    "top_n = 1\n",
    "top_features = feature_imp_df[feature_imp_df['Cumulative Importance'] <= top_n]\n",
    "\n",
    "# Plot feature importance\n",
    "plt.figure(figsize=(12, 8))\n",
    "plt.barh(top_features['Feature'], top_features['Coefficient'])\n",
    "plt.axvline(x=0, color=\"k\", linestyle=\"-\", label=f'Represents Base Price: ${y_intercept:,.2f}')\n",
    "\n",
    "plt.xlabel(f'Positive or Negative Impact of Individual Feature on Base Price', fontdict=axes_fonts)\n",
    "plt.ylabel('Feature by Importance on Price', fontdict=axes_fonts)\n",
    "\n",
    "plt.title(f'Top Features Explaining {top_n * 100:,.0f}% of Price Variance', fontdict=title_fonts)\n",
    "plt.gca().invert_yaxis()\n",
    "plt.savefig('images/feature_importance.png')\n",
    "plt.legend(loc='lower left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad571a42-aab1-42a3-b4b1-aaf7386d25cd",
   "metadata": {},
   "source": [
    "## Segmentation Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cf6dbc0-7c5e-43d0-b48a-0bd0e92d0f60",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.describe().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94e5857b-021d-4e04-ad54-0a96c88ce4cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# logging.getLogger().setLevel(logging.DEBUG)\n",
    "\n",
    "segments = {\n",
    "    'Budget' : data.query('price <= 10000 and year <= 2016 and odometer >= 100000'),\n",
    "    'Entry' : data.query('(price>=10000 and price<=15000) and (year>=2016 and year<=2019) and (odometer>=60000 and odometer<=100000)'),\n",
    "    'Mid' : data.query('(price>=15000 and price<=25000) and (year>=2019 and year<=2021) and (odometer>=40000 and odometer<=60000)'),\n",
    "    'Upper' : data.query('(price>=25000 and price<=35000) and (year>=2021 and year<=2023) and (odometer<=30000)'),\n",
    "    'Premium' : data.query('(price>=35000) and (year>=2022) and (odometer<=20000)')\n",
    "}\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "# seg_preds = {}\n",
    "for segment in segments:\n",
    "    logging.debug(f'Segment: {segment}, #Samples: {len(segments[segment])}')\n",
    "\n",
    "    if len(segments[segment]) <= 0:\n",
    "        continue\n",
    "    \n",
    "    # Predict on the segment\n",
    "    X = segments[segment].drop('price', axis='columns')\n",
    "    y = segments[segment]['price']\n",
    "    logging.debug(f'X: {X.shape} y: {y.shape}')\n",
    "\n",
    "    # transform the data: OneHotEncoding\n",
    "    X = pd.get_dummies(X, drop_first=True)\n",
    "\n",
    "    # Scale the data\n",
    "    X_scaled = scaler.fit_transform(X)\n",
    "    logging.debug(f'X_scaled: {X_scaled.shape}')\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)\n",
    "    logging.debug(f'X_train: {X_train.shape} X_test: {X_test.shape} y_train: {y_train.shape} y_test: {y_test.shape}')\n",
    "\n",
    "    best_model.fit(X_train, y_train)\n",
    "    \n",
    "    # Plot predictions\n",
    "    y_preds = best_model.predict(X_test)\n",
    "    score = best_model.score(X_test, y_test)\n",
    "    r2 = r2_score(y_test, y_preds)\n",
    "    y_intercept = best_model.intercept_\n",
    "    logging.debug(f'Score: {score}, r2: {r2}, y_int: {y_intercept}')\n",
    "    y_preds = y_preds + y_intercept\n",
    "    # seg_preds[segment] = y_preds\n",
    "\n",
    "    perfect_pt_1 = [min(y_test), max(y_test)]\n",
    "    # perfect_pt_2 = [min(y_test), max(y_test)]\n",
    "    perfect_pt_2 = [min(y_test), max(y_test)] + y_intercept\n",
    "    plt.scatter(x=y_test, y=y_preds, label=f'{segment} / ${y_intercept:,.2f}', alpha=0.5)\n",
    "    plt.plot(perfect_pt_1, perfect_pt_2, color='red', linestyle='--')#, label='Theoretical Perfect Predictions')\n",
    "\n",
    "plt.xlabel('Actual Prices', fontdict=axes_fonts)\n",
    "plt.ylabel('Predicted Prices', fontdict=axes_fonts)\n",
    "plt.title(f'Model Predictions vs Actual Prices for Price Segments', fontdict=title_fonts)\n",
    "plt.legend().set_title('Price Segments / Base Price')\n",
    "\n",
    "plt.savefig('images/scatter-segments-preds-v-test.png')\n",
    "plt.show()\n",
    "\n",
    "logging.getLogger().setLevel(logging.INFO)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d02d142e-97b3-4bab-81a9-7174978cda87",
   "metadata": {},
   "source": [
    "#### Single Segment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2129cd5e-a52f-41c9-a151-6260dc9f0433",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict on the segment\n",
    "seg = 'Mid'\n",
    "X = segments[seg].drop('price', axis='columns')\n",
    "y = segments[seg]['price']\n",
    "\n",
    "# Transform the data\n",
    "X = pd.get_dummies(X, drop_first=True)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Scale the data\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.fit_transform(X_test)\n",
    "\n",
    "best_model.fit(X_train_scaled, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d028a0a-9982-44ed-8367-9cf8f2634bba",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X.shape, y.shape, X_train.shape, X_test.shape, y_train.shape, y_test.shape, X_train_scaled.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0faff586-72c0-49d5-bc35-39cf64270db9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot predictions\n",
    "y_preds = best_model.predict(X_test_scaled)\n",
    "score = best_model.score(X_test_scaled, y_test)\n",
    "r2 = r2_score(y_test, y_preds)\n",
    "logging.debug(f'Score: {score}, r2: {r2}')\n",
    "y_intercept = best_model.intercept_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7bbd496-27dd-4605-b591-1188d1858613",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adjust for the y-intercept\n",
    "y_preds = np.add(y_preds, y_intercept)\n",
    "perfect_pt_1 = [min(y_test), max(y_test)]\n",
    "perfect_pt_2 = [min(y_test), max(y_test)] + y_intercept\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.scatter(x=y_test, y=y_preds, label=f'{best_model_name} Model', alpha=0.5)\n",
    "plt.plot(perfect_pt_1, perfect_pt_2, color='red', linestyle='--', label='Theoretical Perfect Predictions')\n",
    "plt.xlabel('Actual Prices', fontdict=axes_fonts)\n",
    "plt.ylabel('Predicted Prices', fontdict=axes_fonts)\n",
    "plt.title(f'Model Predictions vs Actual Prices for the best performing {best_model_name} Model (y-intercept=${y_intercept:,.2f})', \n",
    "          fontdict=title_fonts)\n",
    "plt.legend()\n",
    "plt.savefig('images/scatter-optimized-preds-v-test.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2341eea1-0b65-43a1-ba2c-33a67656acab",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(10, 4))\n",
    "PredictionErrorDisplay.from_estimator(best_model, X_test_scaled, y_test, ax=axes[0], kind=\"actual_vs_predicted\")\n",
    "PredictionErrorDisplay.from_estimator(best_model, X_test_scaled, y_test, ax=axes[1], kind=\"residual_vs_predicted\")\n",
    "\n",
    "plt.suptitle(f'{best_model_name} Model: Comparing Predictions vs Actual Prices (without y-intercept=${y_intercept:,.2f})')\n",
    "# plt.savefig('images/budget_predictions.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "763c2560-0710-4c65-8d6c-2d4d32cb4541",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3be5ee6-ba92-4b41-ac11-21c0406d5627",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
