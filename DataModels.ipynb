{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "363a0b15-b89d-44b5-9e5f-f484aa9eb413",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# What drives the price of a car?\n",
    "\n",
    "![](images/kurt.jpeg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1bce1ca-dc8b-4d24-bb0b-30910f316428",
   "metadata": {},
   "source": [
    "# Overview\n",
    "\n",
    "> **Prompt**: In this application, you will explore a dataset from kaggle. The original dataset contained information on 3 million\n",
    "> used cars. The provided dataset contains information on 426K cars to ensure speed of processing.  Your goal is to\n",
    "> understand what factors make a car more or less expensive.  As a result of your analysis, you should provide\n",
    "> clear recommendations to your client -- a used car dealership -- as to what consumers value in a used car.\n",
    "\n",
    "This was the second Practical Application Project as part of my UC Berkeley ML/AI Professional Certification coursework. \n",
    "\n",
    "There were three requirements for this project:\n",
    "\n",
    "1. Evaluate Linear Regression models for predicting prices for the provided dataset based on the above prompt\n",
    "2. Use the CRISP-DM Framework as the basis for the project breakdown and deliverables\n",
    "3. Provide a business-friendly report for our clients at the used car dealership\n",
    "\n",
    "This Jupyter Notebook follows the CRISP-DM Framework to develop Price Prediction models for our data. \n",
    "\n",
    "In addition:\n",
    "\n",
    "* The initial data investigation is covered in the separate `DataInvestiation.ipynb` notebook and is summarised in the Data section here\n",
    "  * [Local file](DataInvestigation.ipynb)  |  [Github](https://github.com/fazeelgm/UCB_ML_AI_PracticalApp_II/blob/main/DataInvestigation.ipynb)\n",
    "* The Executive Summary in the README for this project covers the final report for the clients\n",
    "\n",
    "~ Fazeel Mufti"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c18249de-b9fd-4a5b-ba75-977c2d16215b",
   "metadata": {},
   "source": [
    "### CRISP-DM Framework\n",
    "\n",
    "<center>\n",
    "    <img src = images/crisp.png width = 50%/>\n",
    "</center>\n",
    "\n",
    "\n",
    "> **Prompt**: To frame the task, throughout our practical applications we will refer back to a standard process\n",
    "> in industry for data projects called CRISP-DM.  This process provides a framework for working through a data\n",
    "> problem.  Your first step in this application will be to read through a brief overview of\n",
    "> CRISP-DM [here](https://mo-pcco.s3.us-east-1.amazonaws.com/BH-PCMLAI/module_11/readings_starter.zip).  After\n",
    "> reading the overview, answer the questions below.\n",
    "\n",
    "The main sections of this document have been structured to follow the above CRISP-DM processes to develop the models for predicting auto prices."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a628f852-0422-4c7e-94b4-0fac8ba68f77",
   "metadata": {},
   "source": [
    "### Business Understanding\n",
    "\n",
    "> **Prompt**: From a business perspective, we are tasked with identifying key drivers for used car prices.  In the\n",
    "> CRISP-DM overview, we are asked to convert this business framing to a data problem definition.  Using a few\n",
    "> sentences, reframe the task as a data task with the appropriate technical vocabulary.\n",
    "\n",
    "The dataset provided by the customer contains prices for 426,880 used cars along with 17 attributes for each vehicle. The business objectives for the project are:\n",
    "\n",
    "1. Provide a price prediction model driven by the feature data\n",
    "2. Explain which vehicle features drive the price of the car\n",
    "\n",
    "I will use the `price` column as the **target or dependent variable**, along with the other feature columns, the **independent variables**, as  inputs to a set of LinearRegression models that will use the inputs to predict the price for each vehicle. Each model will be evaluated on the accuracy of its prediction and the best performing model will be used to _interpret the relative importance of each input features to the price_. To objectively measure each model, the data will be divided into a **training** (80%) and a **test set** (remaining 20%). All the models will be trained on the same training set, and will be evaluated against the Test data with known prices. Before training the models, we will remove noise and incomplete information from the dataset so the models can be trained optimally."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bbd45d5-b216-496a-ab2e-d9e8ea2f5253",
   "metadata": {},
   "source": [
    "# Utilities and Imports\n",
    "\n",
    "This section imports the necessary dependcies and code needed for our PRice Prediction Models - please expand if needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6adff665-041d-4ac8-b56f-f3bf6bf9d9c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Customization imports\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "358b6cc7-8806-45ef-8305-9bd3c37812c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export dataFrame's as images\n",
    "import dataframe_image as dfi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a9f871a2-c164-41df-835f-fe553ad05a1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.compose import make_column_selector, make_column_transformer\n",
    "from sklearn.preprocessing import OneHotEncoder, OrdinalEncoder\n",
    "from sklearn.preprocessing import StandardScaler, RobustScaler, MinMaxScaler\n",
    "\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso\n",
    "from sklearn.dummy import DummyClassifier, DummyRegressor\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0314a970-203d-46e8-a6ad-3924aeec2d70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import my utility methods for this project\n",
    "import utils_practical_2 as my_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "91a2e952-e0fa-4189-b54e-0c8c922293f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure logging\n",
    "import logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(levelname)s: %(message)s')\n",
    "\n",
    "# logging.getLogger().setLevel(logging.DEBUG)\n",
    "# logging.getLogger().setLevel(logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1f850e11-f596-432d-b59d-dba838f9d8b2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Global format rules\n",
    "\n",
    "# Seaborn over-rides\n",
    "sns.set_theme(style='darkgrid')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f32bb593-908d-4f71-b97f-54f8b44dece9",
   "metadata": {},
   "source": [
    "# The Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bb7fdc1-8fe9-4493-a67a-754aa63796bb",
   "metadata": {},
   "source": [
    "## Data Understanding\n",
    "\n",
    "> **Prompt**: After considering the business understanding, we want to get familiar with our data.  Write down\n",
    "> some steps that you would take to get to know the dataset and identify any quality issues within.  Take time\n",
    "> to get to know the dataset and explore what information it contains and how this could be used to inform your\n",
    "> business understanding."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b3a9f98-a2db-47ff-a163-754293416228",
   "metadata": {},
   "source": [
    "**An exploratory data analysis was performed on the Customer dataset and this section summarizes the findings detailed in the companion `DataInvestigation.ipynb` notebook**:\n",
    "* `DataInvestigation.ipynb`: [Local file](DataInvestigation.ipynb)&nbsp;&nbsp;|&nbsp;&nbsp;[Github](https://github.com/fazeelgm/UCB_ML_AI_PracticalApp_II/blob/main/DataInvestigation.ipynb)\n",
    "\n",
    "Looking at our `price` target variable, we find:\n",
    "\n",
    "* There are 32,895 cars with a zero price, 7.71% of the total (426,880) - these are candidates for exclusion pending further analysis\n",
    "* The price distribution shows an extreme right-skew and a long-tail with a few outliers beyond \\\\$100K price point\n",
    "  \n",
    "![](images/price_distribution.png)\n",
    "\n",
    "* Analyzed outliers using two separate methods (quantile analysis and Modified z-score)\n",
    "    * ModZ performed better, i.e. preserved more data samples - removing only 5,790 instead of 8,177 priced vehicles\n",
    "* Looking at the distribution of priced cars, i.e. cars in the sample dataset that have pricing informaion:\n",
    "    * There seems to be **at least a low- and mid-price band with a long tail that represents high-priced autos**\n",
    "\n",
    "Based on Price inspection, a potential hypothesis arose that the used car inventory is _segmented_ based on the following price bands:\n",
    "\n",
    "![](images/candidate-price-segments.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10253732-4619-45c6-a42c-e0503076c48d",
   "metadata": {},
   "source": [
    "In addition, I did some market research on used car sales for 2023 to learn more about this domain:\n",
    "\n",
    "> [Auto Remarketing, July 2023: Mid-Year Market Report: Dissecting used-car pricing, segment trends, CPO sales & more](http://digital.autoremarketing.com/publication/frame.php?i=794065&p=&pn=&ver=html5&view=articleBrowser&article_id=4594802)\n",
    ">\n",
    ">    * \"Average used vehicle transaction price in the first quaretr actually dipped by 6.4% year-over-year to \\\\$28,381, compared to \\\\$30,329 in Q1 2022\"\n",
    "\n",
    "Though we don't have year-over-year sales information in this dataset, market research suggests that typical used cars can be categoriezed into price ranges like Budget, Mid, Luxury, etc. that will be based on feature groups. I looked at ```<price, year, condition, odometer>``` combinations and saw that there was clustering behavior as shown by the scatter plots below:\n",
    "\n",
    "![](images/scatter-price-odo-condition-budget.png)\n",
    "![](images/scatter-price-odo-year-entry.png)\n",
    "\n",
    "Please refer to the Data Investigation for more details, but we will use this information to guide our data modeling and optimization."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6533076-63b5-4c98-9016-4888eeb21a9d",
   "metadata": {},
   "source": [
    "## Data Preparation\n",
    "\n",
    "> **Prompt**: After our initial exploration and fine tuning of the business understanding, it is time to\n",
    "> construct our final dataset prior to modeling.  Here, we want to make sure to handle any integrity issues\n",
    "> and cleaning, the engineering of new features, any transformations that we believe should happen\n",
    "> (scaling, logarithms, normalization, etc.), and general preparation for modeling with `sklearn`. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bad90546-b696-4c35-9366-547582002cff",
   "metadata": {},
   "source": [
    "While investigating the target and independent features of our dataset during the Data Investigation, I learnt that the data is very noisy with extreme outliers. Based on my findings, the `get_cleansed_data()` Python function was created, which: \n",
    "\n",
    "* Reads the input dataset\n",
    "* Remove null data and outliers\n",
    "* Removes unnecessary features like ID, Model and VIN that are specific to individual cars, and are not useful for the kind of machine-learning algorithms that we will be using\n",
    "* Transforms the datatypes to standard Python types that are more conducive for our regression models\n",
    "\n",
    "Here is the DataPreparation Report generated by this function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9894f222-393a-435b-afe9-e8aa86198e06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading data/vehicles.csv ... Done: (426880, 18)\n",
      "\n",
      "Cleansing price column ... \n",
      "... Removing price outliers using ModZ method ... \n",
      "... ModZ: 9450.0, med: 13950.0, const: 0.6745\n",
      "... Time: 0.12331819534301758\n",
      "... Removed 5,790 outliers\n",
      "... Removing cars with price <= 0 ...  Removed 32,895 rows\n",
      "Done: (421090, 19) -> (388195, 19)\n",
      "\n",
      "DropNA from columns: \n",
      "... year: 1,029 rows (0.27% of total): 388,195 -> 387,166\n",
      "... manufacturer: 16,609 rows (4.28% of total): 388,195 -> 371,586\n",
      "... fuel: 19,173 rows (4.94% of total): 388,195 -> 369,022\n",
      "... title_status: 26,730 rows (6.89% of total): 388,195 -> 361,465\n",
      "... odometer: 28,960 rows (7.46% of total): 388,195 -> 359,235\n",
      "... transmission: 30,742 rows (7.92% of total): 388,195 -> 357,453\n",
      "Done: (388195, 19) -> (360700, 19)\n",
      "\n",
      "Dropping columns: ['mod_zscore', 'id', 'model', 'VIN']\n",
      "... mod_zscore\n",
      "... id\n",
      "... model\n",
      "... VIN\n",
      "Done: (360700, 19) -> (360700, 15)\n",
      "\n",
      "Data Transformations:\n",
      "... year float -> int: Done\n",
      "... odometer float -> int: Done\n",
      "\n",
      "Category Transformations:\n",
      "... Converting column \"condition\" -> Category: Done\n",
      "... Converting column \"manufacturer\" -> Category: Done\n",
      "... Converting column \"cylinders\" -> Category: Done\n",
      "... Converting column \"fuel\" -> Category: Done\n",
      "... Converting column \"title_status\" -> Category: Done\n",
      "... Converting column \"state\" -> Category: Done\n",
      "... Converting column \"transmission\" -> Category: Done\n",
      "... Converting column \"drive\" -> Category: Done\n",
      "... Converting column \"size\" -> Category: Done\n",
      "... Converting column \"type\" -> Category: Done\n",
      "... Converting column \"paint_color\" -> Category: Done\n",
      "\n",
      "Returned Raw(426,880x19) and Cleansed(360,700x15) data\n",
      "Dataset reduced by 66,180 rows (preserved 84.50% of total)\n"
     ]
    }
   ],
   "source": [
    "vehicles_raw, vehicles_cleansed = my_utils.get_cleansed_data(cleanse_data=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e93ef0e-ff80-48f1-a883-8c6434e9de7f",
   "metadata": {},
   "source": [
    "In summary, 66,180 samples were dropped, preserving 84.50% of the original data. It is important to note that we were able to preserve a good portion of high-quality data with `price` information. This gives us a good starting point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "bfee5084-49bb-40b1-aa09-a29e63d87c4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 360700 entries, 27 to 426879\n",
      "Data columns (total 15 columns):\n",
      " #   Column        Non-Null Count   Dtype   \n",
      "---  ------        --------------   -----   \n",
      " 0   region        360700 non-null  object  \n",
      " 1   price         360700 non-null  int64   \n",
      " 2   year          360700 non-null  int64   \n",
      " 3   manufacturer  360700 non-null  category\n",
      " 4   condition     223668 non-null  category\n",
      " 5   cylinders     213522 non-null  category\n",
      " 6   fuel          360700 non-null  category\n",
      " 7   odometer      360700 non-null  int64   \n",
      " 8   title_status  360700 non-null  category\n",
      " 9   transmission  360700 non-null  category\n",
      " 10  drive         252582 non-null  category\n",
      " 11  size          104338 non-null  category\n",
      " 12  type          282300 non-null  category\n",
      " 13  paint_color   257706 non-null  category\n",
      " 14  state         360700 non-null  category\n",
      "dtypes: category(11), int64(3), object(1)\n",
      "memory usage: 17.5+ MB\n"
     ]
    }
   ],
   "source": [
    "vehicles_cleansed.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "40e4dc6b-2801-4e63-90ee-ae6a207d4714",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "      <th>unique</th>\n",
       "      <th>top</th>\n",
       "      <th>freq</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>min</th>\n",
       "      <th>25%</th>\n",
       "      <th>50%</th>\n",
       "      <th>75%</th>\n",
       "      <th>max</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>region</th>\n",
       "      <td>360700</td>\n",
       "      <td>404</td>\n",
       "      <td>columbus</td>\n",
       "      <td>3170</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>price</th>\n",
       "      <td>360700.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>18091.568775</td>\n",
       "      <td>13071.727526</td>\n",
       "      <td>1.0</td>\n",
       "      <td>7000.0</td>\n",
       "      <td>14999.0</td>\n",
       "      <td>26990.0</td>\n",
       "      <td>62981.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>year</th>\n",
       "      <td>360700.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2011.178007</td>\n",
       "      <td>9.122806</td>\n",
       "      <td>1900.0</td>\n",
       "      <td>2008.0</td>\n",
       "      <td>2013.0</td>\n",
       "      <td>2017.0</td>\n",
       "      <td>2022.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>manufacturer</th>\n",
       "      <td>360700</td>\n",
       "      <td>42</td>\n",
       "      <td>ford</td>\n",
       "      <td>61351</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>condition</th>\n",
       "      <td>223668</td>\n",
       "      <td>6</td>\n",
       "      <td>good</td>\n",
       "      <td>113394</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cylinders</th>\n",
       "      <td>213522</td>\n",
       "      <td>8</td>\n",
       "      <td>6 cylinders</td>\n",
       "      <td>81791</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fuel</th>\n",
       "      <td>360700</td>\n",
       "      <td>5</td>\n",
       "      <td>gas</td>\n",
       "      <td>308197</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>odometer</th>\n",
       "      <td>360700.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>98543.503208</td>\n",
       "      <td>195802.978631</td>\n",
       "      <td>0.0</td>\n",
       "      <td>39283.0</td>\n",
       "      <td>88632.0</td>\n",
       "      <td>136079.0</td>\n",
       "      <td>10000000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>title_status</th>\n",
       "      <td>360700</td>\n",
       "      <td>6</td>\n",
       "      <td>clean</td>\n",
       "      <td>348127</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>transmission</th>\n",
       "      <td>360700</td>\n",
       "      <td>3</td>\n",
       "      <td>automatic</td>\n",
       "      <td>281732</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>drive</th>\n",
       "      <td>252582</td>\n",
       "      <td>3</td>\n",
       "      <td>4wd</td>\n",
       "      <td>110988</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>size</th>\n",
       "      <td>104338</td>\n",
       "      <td>4</td>\n",
       "      <td>full-size</td>\n",
       "      <td>54756</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>type</th>\n",
       "      <td>282300</td>\n",
       "      <td>13</td>\n",
       "      <td>sedan</td>\n",
       "      <td>75099</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>paint_color</th>\n",
       "      <td>257706</td>\n",
       "      <td>12</td>\n",
       "      <td>white</td>\n",
       "      <td>66740</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>state</th>\n",
       "      <td>360700</td>\n",
       "      <td>51</td>\n",
       "      <td>ca</td>\n",
       "      <td>40411</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 count unique          top    freq          mean  \\\n",
       "region          360700    404     columbus    3170           NaN   \n",
       "price         360700.0    NaN          NaN     NaN  18091.568775   \n",
       "year          360700.0    NaN          NaN     NaN   2011.178007   \n",
       "manufacturer    360700     42         ford   61351           NaN   \n",
       "condition       223668      6         good  113394           NaN   \n",
       "cylinders       213522      8  6 cylinders   81791           NaN   \n",
       "fuel            360700      5          gas  308197           NaN   \n",
       "odometer      360700.0    NaN          NaN     NaN  98543.503208   \n",
       "title_status    360700      6        clean  348127           NaN   \n",
       "transmission    360700      3    automatic  281732           NaN   \n",
       "drive           252582      3          4wd  110988           NaN   \n",
       "size            104338      4    full-size   54756           NaN   \n",
       "type            282300     13        sedan   75099           NaN   \n",
       "paint_color     257706     12        white   66740           NaN   \n",
       "state           360700     51           ca   40411           NaN   \n",
       "\n",
       "                        std     min      25%      50%       75%         max  \n",
       "region                  NaN     NaN      NaN      NaN       NaN         NaN  \n",
       "price          13071.727526     1.0   7000.0  14999.0   26990.0     62981.0  \n",
       "year               9.122806  1900.0   2008.0   2013.0    2017.0      2022.0  \n",
       "manufacturer            NaN     NaN      NaN      NaN       NaN         NaN  \n",
       "condition               NaN     NaN      NaN      NaN       NaN         NaN  \n",
       "cylinders               NaN     NaN      NaN      NaN       NaN         NaN  \n",
       "fuel                    NaN     NaN      NaN      NaN       NaN         NaN  \n",
       "odometer      195802.978631     0.0  39283.0  88632.0  136079.0  10000000.0  \n",
       "title_status            NaN     NaN      NaN      NaN       NaN         NaN  \n",
       "transmission            NaN     NaN      NaN      NaN       NaN         NaN  \n",
       "drive                   NaN     NaN      NaN      NaN       NaN         NaN  \n",
       "size                    NaN     NaN      NaN      NaN       NaN         NaN  \n",
       "type                    NaN     NaN      NaN      NaN       NaN         NaN  \n",
       "paint_color             NaN     NaN      NaN      NaN       NaN         NaN  \n",
       "state                   NaN     NaN      NaN      NaN       NaN         NaN  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vehicles_cleansed.describe(include='all').T"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52e26859-b701-4356-b412-d50c8e7ea25c",
   "metadata": {},
   "source": [
    "This is our candidate dataset that we will use for testing out different Pricing Models!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c54a9e63-0dae-45a9-a9f8-f2875be36cd1",
   "metadata": {},
   "source": [
    "# Modeling\n",
    "\n",
    "> **Prompt**: With your (almost?) final dataset in hand, it is now time to build some models.  Here, you\n",
    "> should build a number of different regression models with the price as the target.  In building your\n",
    "> models, you should explore different parameters and be sure to cross-validate your findings."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d77117c-469c-4bf8-895b-99daeb18db79",
   "metadata": {},
   "source": [
    "## Feature Preparation for Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4f58f92-5c4e-4ff6-942b-4f9fbe32a426",
   "metadata": {},
   "source": [
    "I will now evaluate the cleansed data to analyze the features for model training.\n",
    "\n",
    "But first, let's make a copy of the cleansed data so the _clean data_ is always available for comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f2ee0ea1-5830-4599-b58f-81138e76b0bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a copy of the clean data\n",
    "data = vehicles_cleansed.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c402754d-825c-486e-ac92-4cd54c5473a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 360700 entries, 27 to 426879\n",
      "Data columns (total 15 columns):\n",
      " #   Column        Non-Null Count   Dtype   \n",
      "---  ------        --------------   -----   \n",
      " 0   region        360700 non-null  object  \n",
      " 1   price         360700 non-null  int64   \n",
      " 2   year          360700 non-null  int64   \n",
      " 3   manufacturer  360700 non-null  category\n",
      " 4   condition     223668 non-null  category\n",
      " 5   cylinders     213522 non-null  category\n",
      " 6   fuel          360700 non-null  category\n",
      " 7   odometer      360700 non-null  int64   \n",
      " 8   title_status  360700 non-null  category\n",
      " 9   transmission  360700 non-null  category\n",
      " 10  drive         252582 non-null  category\n",
      " 11  size          104338 non-null  category\n",
      " 12  type          282300 non-null  category\n",
      " 13  paint_color   257706 non-null  category\n",
      " 14  state         360700 non-null  category\n",
      "dtypes: category(11), int64(3), object(1)\n",
      "memory usage: 17.5+ MB\n"
     ]
    }
   ],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcfd58a6-b008-4954-8c08-16aae66d05eb",
   "metadata": {},
   "source": [
    "Evaluating features, we see that `region`, `manufacturer`, `paint_color` and `state` have interesting information, but porbably not useful for overall pricing at this stage - we are not developing a regionally-optimized model and want to be able to predict pricing over a large population of used cars. We will drop these features from model training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "aecbd077-08d4-40a9-9232-15bae5d5f945",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove unwanted columns\n",
    "drop_cols = ['region', 'manufacturer', 'paint_color', 'state']\n",
    "data.drop(columns=drop_cols, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6da8831-24f4-4feb-b187-76c07da49ebc",
   "metadata": {},
   "source": [
    "Let's look at null-data values now ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "9ea9ddc3-2b82-41a6-8ecd-3d262db0a4c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percent non-null values in each column\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "price           100.00\n",
       "year            100.00\n",
       "condition        67.90\n",
       "cylinders        65.52\n",
       "fuel            100.00\n",
       "odometer        100.00\n",
       "title_status    100.00\n",
       "transmission    100.00\n",
       "drive            74.67\n",
       "size             39.95\n",
       "type             81.63\n",
       "dtype: float64"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('Percent non-null values in each column')\n",
    "data.isnull().sum().transform(lambda x: (1 - (x / vehicles_raw.shape[0])) * 100).round(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "2ec7afc0-b80c-4115-b397-002f3b065213",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data with nulls - shape(282074, 11): 78.20% of cleansed data\n",
      "Data without nulls - shape(78626, 11): 21.80% of cleansed data\n"
     ]
    }
   ],
   "source": [
    "# DataFrame containing rows with any null values\n",
    "data_with_nulls = data[data.isnull().any(axis=1)]\n",
    "\n",
    "# DataFrame containing rows with all non-null values\n",
    "data_without_nulls = data[~data.isnull().any(axis=1)]\n",
    "\n",
    "# Print the resulting DataFrames\n",
    "print('Data with nulls - shape{}: {:,.2f}% of cleansed data'.format(data_with_nulls.shape, (data_with_nulls.shape[0] / vehicles_cleansed.shape[0]) * 100))\n",
    "print('Data without nulls - shape{}: {:,.2f}% of cleansed data'.format(data_without_nulls.shape, (data_without_nulls.shape[0] / vehicles_cleansed.shape[0]) * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a765ebb-5c80-488e-aeef-0306b0cb2df7",
   "metadata": {},
   "source": [
    "Let's remove the nulls from the data to improve model training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "c549e347-221b-4476-a120-f89cefc2338b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dropping nulls: (360700, 11) ->  (122682, 11): 65.99% reduction\n"
     ]
    }
   ],
   "source": [
    "# remove nulls before splitting data\n",
    "\n",
    "dropna_cols = ['condition', 'cylinders', 'drive', 'type']\n",
    "\n",
    "print('Dropping nulls: {} -> '.format(data.shape), end='')\n",
    "# data.dropna(subset=['condition', 'cylinders', 'drive', 'size', 'type'], axis='index', inplace=True)\n",
    "data.dropna(subset=dropna_cols, axis='index', inplace=True)\n",
    "print(' {}: {:,.2f}% reduction'.format(data.shape, (1 - (data.shape[0] / vehicles_cleansed.shape[0])) * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "ae860574-3cff-4b83-9a4c-612ef3a10eb0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['price', 'year', 'condition', 'cylinders', 'fuel', 'odometer',\n",
       "       'title_status', 'transmission', 'drive', 'size', 'type'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ac0fed3-e7b3-463b-8911-5e3fbfd80461",
   "metadata": {},
   "source": [
    "### Create Train/Test splits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6274207-72ce-44af-a503-099b1a28ba9c",
   "metadata": {},
   "source": [
    "We will now create two feature matrices for the independent (X) and target (y) variables., and then split them into Train / Test data sets after encoding all categorical data so the regression models can accept it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "2fb73893-c7e5-4328-99cb-873ac9978593",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the feature matrices for independnet and target variables\n",
    "\n",
    "X = data.drop('price', axis='columns')\n",
    "y = data['price']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "8803a3a1-5eb6-4508-a2b0-24594b3d5cea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# OneHot Encode the features and drop the first value to reduce multicollinearity\n",
    "X = pd.get_dummies(X, drop_first=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "3ee82eb1-897f-4fed-80ad-6247884a3df9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure that we have consistent random-state for all models, otherwise the vary across runs\n",
    "random_state = 13"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "36c68ddf-16c0-49c0-bc8c-bf0976eb3755",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=random_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "be3c897e-691a-4954-b786-88ab9e08a3de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>31</th>\n",
       "      <th>32</th>\n",
       "      <th>33</th>\n",
       "      <th>34</th>\n",
       "      <th>35</th>\n",
       "      <th>36</th>\n",
       "      <th>38</th>\n",
       "      <th>42</th>\n",
       "      <th>45</th>\n",
       "      <th>55</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>year</th>\n",
       "      <td>2013</td>\n",
       "      <td>2012</td>\n",
       "      <td>2016</td>\n",
       "      <td>2019</td>\n",
       "      <td>2016</td>\n",
       "      <td>2011</td>\n",
       "      <td>2017</td>\n",
       "      <td>2016</td>\n",
       "      <td>2018</td>\n",
       "      <td>2004</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>odometer</th>\n",
       "      <td>128000</td>\n",
       "      <td>68696</td>\n",
       "      <td>29499</td>\n",
       "      <td>43000</td>\n",
       "      <td>17302</td>\n",
       "      <td>30237</td>\n",
       "      <td>30041</td>\n",
       "      <td>9704</td>\n",
       "      <td>37332</td>\n",
       "      <td>88000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>condition_fair</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>condition_good</th>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>condition_like new</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>condition_new</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>condition_salvage</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cylinders_12 cylinders</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cylinders_3 cylinders</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cylinders_4 cylinders</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            31     32     33     34     35     36     38  \\\n",
       "year                      2013   2012   2016   2019   2016   2011   2017   \n",
       "odometer                128000  68696  29499  43000  17302  30237  30041   \n",
       "condition_fair           False  False  False  False  False  False  False   \n",
       "condition_good           False   True   True  False   True   True   True   \n",
       "condition_like new       False  False  False  False  False  False  False   \n",
       "condition_new            False  False  False  False  False  False  False   \n",
       "condition_salvage        False  False  False  False  False  False  False   \n",
       "cylinders_12 cylinders   False  False  False  False  False  False  False   \n",
       "cylinders_3 cylinders    False  False  False  False  False  False  False   \n",
       "cylinders_4 cylinders    False  False  False  False  False  False  False   \n",
       "\n",
       "                           42     45     55  \n",
       "year                     2016   2018   2004  \n",
       "odometer                 9704  37332  88000  \n",
       "condition_fair          False  False  False  \n",
       "condition_good           True   True   True  \n",
       "condition_like new      False  False  False  \n",
       "condition_new           False  False  False  \n",
       "condition_salvage       False  False  False  \n",
       "cylinders_12 cylinders  False  False  False  \n",
       "cylinders_3 cylinders   False  False  False  \n",
       "cylinders_4 cylinders   False  False  False  "
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# spot-check feature encoding\n",
    "X.T.iloc[0:10, 0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c13a99ac-6550-4aad-a8e4-5f488f99df26",
   "metadata": {},
   "source": [
    "### Feature Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "92063183-92e5-42d3-862f-a95852ba03b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale the data - we'll use StandardScaler for the baseline model\n",
    "logging.debug('Scaling data')\n",
    "scaler = StandardScaler()\n",
    "\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.fit_transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7770b4e-e9dd-42c2-9556-c440ab883821",
   "metadata": {},
   "source": [
    "## Baseline Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "412cc524-cac0-4ff4-aafa-ca222d0319cf",
   "metadata": {},
   "source": [
    "We will use the `DummyRegressor` as the baseline price prediction model, using a simple strategy of always using the `mean` price of the training set as the predicted price."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "6329bdf9-bee8-4047-998f-d8cbf8771ef3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The mean price for the training dataset is : $15,690.10\n"
     ]
    }
   ],
   "source": [
    "y_train_mean_price = y_train.mean()\n",
    "print(f'The mean price for the training dataset is : ${y_train_mean_price:,.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64f69a81-62fd-4ba0-8a14-4fe2488f4ecb",
   "metadata": {},
   "source": [
    "Please note that the code below will use a helper method `get_model_metrics_as_results()` that is defined in the `utils_practical_2.py` Python file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "8d5f25f4-54e2-4ad4-a209-9fe9459c85fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function get_model_metrics_as_results in module utils_practical_2:\n",
      "\n",
      "get_model_metrics_as_results(name, clf, scaler, X_test, y_test, override_rmse=0)\n",
      "    Build standardized results row given the predictions and y_test values\n",
      "    \n",
      "    :param name: Model name for labeling the row in the table\n",
      "    :param clf: Fitted classifier to get metrics\n",
      "    :param scaler: Fitted scaler used for transformation\n",
      "    :param X_test: Test data used to fit the classifier\n",
      "    :param y_test: Evaluation data to be used for the metrics\n",
      "    :param override_rmse: Default 0 will internally calculate RMSE as np.sqrt(MSE), pass in a \n",
      "        value for cross-validated estimator result if needed\n",
      "    :return: Returns single row of results summary table containing:\n",
      "    \n",
      "        [model_name, MAE, MSE, RMSE, R2_Score, y-intercept]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(my_utils.get_model_metrics_as_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "a59b2fd5-fee4-4d84-a424-8d7b7b0cfb1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup the linear models\n",
    "models = {\n",
    "    'DummyRegressor' : DummyRegressor(strategy='mean'),\n",
    "    # 'LinearRegression' : LinearRegression(),\n",
    "    # 'Ridge' : Ridge(alpha=1.0),\n",
    "    # 'Lasso' : Lasso(alpha=0.1, max_iter=2000)\n",
    "}\n",
    "\n",
    "# save results for tabulation\n",
    "results_baseline = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "0718d7ce-19fa-4e61-9cbb-d7f7d6fcf151",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working on DummyRegressor ... Done with DummyRegressor (0.0209 sec)\n"
     ]
    }
   ],
   "source": [
    "# Run the baseline models\n",
    "\n",
    "# logging.getLogger().setLevel(logging.DEBUG)\n",
    "\n",
    "results_baseline = []\n",
    "\n",
    "# iterate over the models and build results DF\n",
    "for model in models:\n",
    "\n",
    "    start_time = time.time()\n",
    "    print(f'Working on {model} ... ', end='')\n",
    "\n",
    "    # Fit the model\n",
    "    models[model].fit(X_train_scaled, y_train)\n",
    "    \n",
    "    # Append model metrics to results\n",
    "    results_baseline.append(my_utils.get_model_metrics_as_results(model, models[model], scaler, X_test_scaled, y_test))\n",
    "    \n",
    "    print(f'Done with {model} ({time.time() - start_time:,.4f} sec)')\n",
    "\n",
    "# logging.getLogger().setLevel(logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "98e2bb0c-e5d6-4554-9e65-64a6b0bbf35e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RESULTS: Baseline metrics without any optimization (Sorted: max(R2), min(MAE)\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Scaler</th>\n",
       "      <th>MAE</th>\n",
       "      <th>MSE</th>\n",
       "      <th>RMSE</th>\n",
       "      <th>R2 Score</th>\n",
       "      <th>Base Price</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Model</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>DummyRegressor</th>\n",
       "      <td>StandardScaler</td>\n",
       "      <td>10,125.928777</td>\n",
       "      <td>152,738,375.600850</td>\n",
       "      <td>12,358.736813</td>\n",
       "      <td>-0.000027</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        Scaler            MAE                 MSE  \\\n",
       "Model                                                               \n",
       "DummyRegressor  StandardScaler  10,125.928777  152,738,375.600850   \n",
       "\n",
       "                         RMSE   R2 Score  Base Price  \n",
       "Model                                                 \n",
       "DummyRegressor  12,358.736813  -0.000027           0  "
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Generate results table\n",
    "results_baseline_df = pd.DataFrame(results_baseline, \n",
    "                                   columns=['Model', 'Scaler', 'MAE', 'MSE', 'RMSE', 'R2 Score', 'Base Price']\n",
    "                                  ).set_index('Model')\n",
    "\n",
    "# Sort by highest R2, lowest MAE\n",
    "results_baseline_df.sort_values(by=['R2 Score', 'MAE'], ascending=[False, True], inplace=True)\n",
    "\n",
    "# Export results for README\n",
    "results_baseline_df_styled = my_utils.df_style_floats(results_baseline_df)\n",
    "dfi.export(results_baseline_df_styled, 'images/results_baseline_table.png')\n",
    "\n",
    "print('RESULTS: Baseline metrics without any optimization (Sorted: max(R2), min(MAE)\\n')\n",
    "results_baseline_df_styled"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3a6b2a7-7476-4564-a6a2-b55e1bc7a467",
   "metadata": {},
   "source": [
    "As expected, the R2 score of near 0 shows that the model is no better than a always predicting the mean of the target variable `price`. The slight negative value suggests that our model is slightly worse than simply predicting the mean. There is not much point in interpreting the rest of the metrics at this point."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adebdccf-fb94-404c-951e-5547112d66b8",
   "metadata": {},
   "source": [
    "## Model Tuning: Hyperparameter Tuning and Cross Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c665c77b-b9bf-422c-8da2-e2a6aacb100d",
   "metadata": {},
   "source": [
    "Now that we have established a baseline, I will use the following standard linear regression models to train our data:\n",
    "\n",
    "* LinearRegression Model: Provides a baseline for a commercial-strength model, but with no regularization. It should perform well if our dataset is not prone to overfitting\n",
    "* Lasso (L1 Regularization) Model: Helpful in eliminating less important features, and can be used for feature selection\n",
    "* Ridge (L2 Regularization) Model: Often performs better than linear regression when multicollinearity is present, as it penalizes large coefficients\n",
    "\n",
    "In order to find the most suitable model for our data, I will use the following procedure to find the best model:\n",
    "\n",
    "* Hyperparameter Tuning: GridSearchCV is used to find the best alpha parameter for Ridge and Lasso models. The grid search is performed with 5-fold cross-validation\n",
    "* Cross-validation: cross_val_score is used to evaluate the models' performance with cross-validation, providing a more robust estimate of model performance\n",
    "* Model Evaluation: The models are evaluated using Mean Absolute Error (MAE), Mean Squared Error (MSE), and R-squared metrics on the test set\n",
    "* Scaling: Use different scaling techniques to bring all the features closer to a similar or same range or scale, so they contribute equally to price prediction. Some models also converge quicker with scaled data\n",
    "  * StandardScaler: Results in a distribution with 0 mean and 1 standard deviation. Sensitive to outliers\n",
    "  * MinMaxScaler: Restricts value range to [0,1]. Sensitive to outliers\n",
    "  * RobustScaler: Removes the median and uses the interquartile range (IQR) as a reference point for scaling. It is not affected by outliers\n",
    "\n",
    "I will then compare the performance of these models with hyperparameter tuning and cross-validation, providing insights into which model is best suited for predicting car prices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "929bacba-0403-4875-8331-2b28dff64c87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the data structures to loop on for optimization\n",
    "models = {\n",
    "    'LinearRegression': LinearRegression(),\n",
    "    'Lasso': Lasso(),\n",
    "    'Ridge': Ridge()\n",
    "}\n",
    "\n",
    "# hyperparameter settings that we would like to tune for each model\n",
    "cv_param_grid = {\n",
    "    'Ridge': {'alpha': [0.1, 1.0, 10.0, 100.0], 'random_state': [random_state]},\n",
    "    'Lasso': {'alpha': [0.01, 0.1, 1.0, 10.0], 'random_state': [random_state], 'max_iter': [2000] }    \n",
    "}\n",
    "\n",
    "# List of scalers we want to try\n",
    "scalers = {\n",
    "    StandardScaler(),\n",
    "    RobustScaler(),    # default: quantile_range=(25, 75)\n",
    "    MinMaxScaler()\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "d343d7b2-6ffa-448d-be6a-ea08bcb5516c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Toggle debugging output\n",
    "# logging.getLogger().setLevel(logging.DEBUG)\n",
    "# logging.getLogger().setLevel(logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "09c47a07-353d-4f88-b5da-60e7069d954a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working on MinMaxScaler() ... \n",
      "\tWorking on LinearRegression, Scaler: MinMaxScaler()\n",
      "\tDone with LinearRegression (4.5714 sec)\n",
      "\tWorking on Lasso: {'alpha': [0.01, 0.1, 1.0, 10.0], 'random_state': [13], 'max_iter': [2000]}, Scaler: MinMaxScaler()\n",
      "\tDone with Lasso (30.8186 sec)\n",
      "\tWorking on Ridge: {'alpha': [0.1, 1.0, 10.0, 100.0], 'random_state': [13]}, Scaler: MinMaxScaler()\n",
      "\tDone with Ridge (0.8664 sec)\n",
      "Done with MinMaxScaler()\n",
      "Working on StandardScaler() ... \n",
      "\tWorking on LinearRegression, Scaler: StandardScaler()\n",
      "\tDone with LinearRegression (0.7658 sec)\n",
      "\tWorking on Lasso: {'alpha': [0.01, 0.1, 1.0, 10.0], 'random_state': [13], 'max_iter': [2000]}, Scaler: StandardScaler()\n",
      "\tDone with Lasso (23.1111 sec)\n",
      "\tWorking on Ridge: {'alpha': [0.1, 1.0, 10.0, 100.0], 'random_state': [13]}, Scaler: StandardScaler()\n",
      "\tDone with Ridge (0.8808 sec)\n",
      "Done with StandardScaler()\n",
      "Working on RobustScaler() ... \n",
      "\tWorking on LinearRegression, Scaler: RobustScaler()\n",
      "\tDone with LinearRegression (0.7498 sec)\n",
      "\tWorking on Lasso: {'alpha': [0.01, 0.1, 1.0, 10.0], 'random_state': [13], 'max_iter': [2000]}, Scaler: RobustScaler()\n",
      "\tDone with Lasso (21.4353 sec)\n",
      "\tWorking on Ridge: {'alpha': [0.1, 1.0, 10.0, 100.0], 'random_state': [13]}, Scaler: RobustScaler()\n",
      "\tDone with Ridge (0.8503 sec)\n",
      "Done with RobustScaler()\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "# Set loops to cross-validate over model / scaler combinations using GridSearchCV\n",
    "#\n",
    "\n",
    "# save results for tabulation\n",
    "results_tuned = []\n",
    "best_models = {}\n",
    "best_params = {}\n",
    "\n",
    "# Cross-validation count\n",
    "cv=5\n",
    "verbosity=0\n",
    "\n",
    "# Loop thru each scaler and transform the data\n",
    "for scaler in scalers:\n",
    "\n",
    "    print(f'Working on {scaler} ... ')\n",
    "    logging.debug(f'Working on {scaler}')\n",
    "\n",
    "    # Scale X_train\n",
    "    logging.debug(f'Scaling data using {scaler}')\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_test_scaled = scaler.fit_transform(X_test)\n",
    "\n",
    "    # Loop thru each model, fit/predict and store the results\n",
    "    for model in models:\n",
    "\n",
    "        start_time = time.time()\n",
    "\n",
    "        # print(f'\\tWorking on {model} ... ', end='')\n",
    "\n",
    "        clf = models[model]\n",
    "\n",
    "        # Handle CV models\n",
    "        if (model in cv_param_grid):\n",
    "            print('\\tWorking on {}: {}, Scaler: {}'.format(model, cv_param_grid[model], scaler))\n",
    "            grid = GridSearchCV(clf, param_grid=cv_param_grid[model], \n",
    "                                cv=cv, scoring='neg_mean_squared_error', verbose=verbosity)\n",
    "            grid.fit(X_train_scaled, y_train)\n",
    "              \n",
    "            # Save the tuning metrics for this model class\n",
    "            best_models[model] = grid.best_estimator_\n",
    "            best_params[model] = grid.best_params_\n",
    "            logging.debug(f'Best param: {grid.best_params_}')            \n",
    "            rmse = np.sqrt(-grid.best_score_)\n",
    "\n",
    "        # Handle non-CV models\n",
    "        else:\n",
    "            print('\\tWorking on {}, Scaler: {}'.format(model, scaler))\n",
    "            clf.fit(X_train_scaled, y_train)\n",
    "            cv_scores = cross_val_score(clf, X_train_scaled, y_train, cv=cv, scoring='neg_mean_squared_error')\n",
    "\n",
    "            # Save the tuning metrics for this model class\n",
    "            best_models[model] = clf\n",
    "            best_params[model] = ''\n",
    "            rmse = np.sqrt(-np.mean(cv_scores))\n",
    "\n",
    "        y_preds = best_models[model].predict(X_test_scaled)\n",
    "\n",
    "        # Get the predictions for this model instance\n",
    "        score = best_models[model].score(X_test_scaled, y_test)\n",
    "        r2 = r2_score(y_test, y_preds)\n",
    "        logging.debug(f'CV Best RMSE: {rmse}')\n",
    "        logging.debug(f'Score: {score}, r2: {r2}')\n",
    "       \n",
    "        # Append model metrics to results\n",
    "        results_tuned.append(my_utils.get_model_metrics_as_results(model, best_models[model], scaler, X_test_scaled, y_test, override_rmse=rmse))\n",
    "\n",
    "        print(f'\\tDone with {model} ({time.time() - start_time:,.4f} sec)')\n",
    "\n",
    "    print(f'Done with {scaler}')\n",
    "\n",
    "logging.getLogger().setLevel(logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f605bd59-1723-4063-b555-528709cdaa15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate results table\n",
    "results_tuned_df = pd.DataFrame(results_tuned,\n",
    "                                columns=['Model - Tuned', 'Scaler', 'MAE', 'MSE', 'RMSE', 'R2 Score', 'Base Price']\n",
    "                               ).set_index('Model - Tuned')\n",
    "\n",
    "# Add BestParams as a new column\n",
    "results_tuned_df['CV Best Params'] = results_tuned_df.index.map(best_params)\n",
    "\n",
    "# Sort by highest R2, lowest MAE\n",
    "results_tuned_df.sort_values(by=['R2 Score', 'MAE'], ascending=[False, True], inplace=True)\n",
    "\n",
    "# Export results for README\n",
    "results_tuned_df_styled = my_utils.df_style_floats(results_tuned_df)\n",
    "dfi.export(results_tuned_df_styled, 'images/results_tuned_table.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c5ef09b-2b40-4a70-9903-d34322f80741",
   "metadata": {},
   "source": [
    "# Evaluation\n",
    "\n",
    "> **Prompt**: With some modeling accomplished, we aim to reflect on what we identify as a high quality model\n",
    "> and what we are able to learn from this.  We should review our business objective and explore how well we\n",
    "> can provide meaningful insight on drivers of used car prices.  Your goal now is to distill your findings\n",
    "> and determine whether the earlier phases need revisitation and adjustment or if you have information of\n",
    "> value to bring back to your client."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4137b4d-4ac1-4196-a0fd-493263cc8514",
   "metadata": {},
   "source": [
    "## Picking the Best Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91ffba6d-972c-4033-a6fc-70f2fe08edf1",
   "metadata": {},
   "source": [
    "Now that we have the results for our baseline and candidate models in a standardized format, let's evaluate them on performance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85ef569b-d193-4624-b642-dbe7b218cf0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('RESULTS: Baseline Model metrics without any optimization (Sorted: max(R2), min(MAE)\\n')\n",
    "results_baseline_df_styled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "416fb7b5-41d5-4962-abf6-bf0f496bdaba",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'RESULTS: Optimized Model results after hyperparameter tuning and cross-validation ({cv}-fold): Sorted max(R2), min(MAE)\\n')\n",
    "results_tuned_df_styled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "080be12d-d864-4520-b028-4f751fdb2b4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model_idx = 0   # ASSUMES SORTED LIST\n",
    "results_row = results_tuned_df.iloc[best_model_idx]\n",
    "best_model_name = results_tuned_df.index[best_model_idx]\n",
    "best_model = best_models[best_model_name]\n",
    "y_intercept = np.float64(results_row['Base Price'].replace(',',''))\n",
    "\n",
    "print('Best Model with highest R2 Score: {}\\n\\tBest Params: {}\\n\\tScaler: {}\\n\\tR2 Score: {:,.2f}%\\n\\tRMSE: ${:,.2f}\\n\\tBase Price (y-intercept): ${:,.2f}'\n",
    "      .format(best_model_name,\n",
    "              results_row['CV Best Params'],\n",
    "              results_row['Scaler'],\n",
    "              float(results_row['R2 Score']) * 100, \n",
    "              float(results_row['RMSE'].replace(',','')),\n",
    "              y_intercept))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fc417a4-7894-4aa0-9e30-126a533add6e",
   "metadata": {},
   "source": [
    "We see that all the models performed very closely without any clear winners, with **the best R2 score of 52.07% from the tuned Ridge regression model**. Compared to the `DummyRegressor` as our baseline where (R2 = 0) it performed the same as simply predicting the mean of all prices, our tuned Ridge model explains 52% of the variability when predicting the car price from the features, with a standard deviation of $8,496.24 (RMSE: root mean squared error) from the actual price. This is a good start!\n",
    "\n",
    "We can also make the following observations:\n",
    "\n",
    "* The base price of a car is given by the y-intercept for the linear regression model: $15,690.10\n",
    "* Each training feature contributes to the predicted price based on the weights for that specific feature that the model has learned from the training data - we will investigate this further later\n",
    "\n",
    "**_We will use the best performing Ridge model as the basis of our evaluation from this point forward_**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03141659-c776-4a34-a56d-1d5b0398d03d",
   "metadata": {},
   "source": [
    "### Visualizing Actual vs Predicted Prices"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc3c702f-6747-4194-9165-fc49c009126f",
   "metadata": {},
   "source": [
    "Let's visualize our predicted prices relative to the actual prices in the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfdbff29-0581-465c-adc2-5db76699bfe4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot predictions\n",
    "y_preds = best_model.predict(X_test_scaled)\n",
    "\n",
    "# Adjust for the y-intercept\n",
    "y_preds = np.add(y_preds, y_intercept)\n",
    "perfect_pt_1 = [min(y_test), max(y_test)]\n",
    "perfect_pt_2 = [min(y_test), max(y_test)] + y_intercept\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.scatter(x=y_test, y=y_preds, label=f'{best_model_name} Model', alpha=0.5)\n",
    "plt.plot(perfect_pt_1, perfect_pt_2, color='red', linestyle='--', label='Theoretical Perfect Predictions')\n",
    "\n",
    "plt.xlabel('Actual Prices', fontdict=my_utils.axes_fonts)\n",
    "plt.ylabel('Predicted Prices', fontdict=my_utils.axes_fonts)\n",
    "plt.title(f'Model Predictions vs Actual Prices for the best performing {best_model_name} Model (y-intercept=${y_intercept:,.2f})', \n",
    "          fontdict=my_utils.title_fonts)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.savefig('images/scatter-optimized-preds-v-test.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ef85c01-53f8-4923-9d63-e06b8c31cc03",
   "metadata": {},
   "source": [
    "The red line represents the ideal case if all our predictions perfectly matched the actual prices. From the scatter plot above, we can see that our model has learnt the basic pricing relationship from our features but is not performing well on the top and bottom of the price range. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "442dc4b0-d284-4685-8967-19ff17037b0b",
   "metadata": {},
   "source": [
    "### PredictionErrorDisplay: Residuals Comparison"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e11decc9-7b73-4fb1-b5bd-1c95b999868d",
   "metadata": {},
   "source": [
    "Let's look at our prediction data in a different manner to understand how much of the price variability is being captured by our features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "472e687a-3adb-4e9b-a893-8105500ba542",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # How to get the list of available Displays for my version of sklearn\n",
    "# from sklearn.utils.discovery import all_displays\n",
    "# displays = all_displays()\n",
    "# displays\n",
    "\n",
    "from sklearn.metrics import PredictionErrorDisplay\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 6))\n",
    "PredictionErrorDisplay.from_estimator(best_model, X_test_scaled, y_test, ax=axes[0], kind=\"actual_vs_predicted\")\n",
    "PredictionErrorDisplay.from_estimator(best_model, X_test_scaled, y_test, ax=axes[1], kind=\"residual_vs_predicted\")\n",
    "\n",
    "plt.suptitle(f'Best Model: Comparing Predictions vs Actual Prices (without y-intercept=${y_intercept:,.2f})')\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.savefig('images/optimized-PredictionErrorDisplay.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c333644b-9511-4b21-9e3e-f8600316d922",
   "metadata": {},
   "source": [
    "The graph on the left is showing us that the for the high-end, the prediction error increases from the perfect center line. Ideally, the scatter points would hug the diagonal. The graph on the right shows us the residuals, the difference between the actual and the predicted value, and shows us that the model is performing better in the middle price ranges. \n",
    "\n",
    "Taken together, this shows us that we should perhaps use non-linear regresssion methods for this dataset to capture the variation on each end of the price spectrum. However, for the purpose of this study, we're deliberately restricting ourselves to linear regression models to learn more about them."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13ab8887-176c-4a0a-9a64-4d4964ee3ee4",
   "metadata": {},
   "source": [
    "## Interpreting the Best Model Results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd58285d-309a-4d36-a0cc-d0aa073f4f29",
   "metadata": {},
   "source": [
    "#### Feature Importance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "343e6c34-23d3-4b8a-98d5-ab707dacc6c1",
   "metadata": {},
   "source": [
    "We now switch our focus to interpreting the results of the best performing model and analyze the coefficients, or weightings, for each feature that were _learnt_ by our model during the training process. This represents the causal relationship between that feature and the price of the vehicle, or the importance of that feature to the price prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb146a95-8f9a-4933-96f9-e3fd6dc2c662",
   "metadata": {},
   "outputs": [],
   "source": [
    "# logging.getLogger().setLevel(logging.DEBUG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2f7b5df-1d7d-4d95-a909-7c5d6fdbd834",
   "metadata": {},
   "outputs": [],
   "source": [
    "# build the feature importance DF\n",
    "\n",
    "feature_names = X.columns\n",
    "\n",
    "feature_imp = []\n",
    "for feature, coef in zip(feature_names, best_model.coef_):\n",
    "    if coef != 0:\n",
    "        # print(f'{feature}: {coef}')\n",
    "        feature_imp.append([feature, coef])\n",
    "\n",
    "feature_imp_df = pd.DataFrame(data=feature_imp, columns=['Feature', 'Coefficient'])#.set_index('Feature')\n",
    "feature_imp_df['Importance'] = np.abs(feature_imp_df.Coefficient)\n",
    "\n",
    "# Sort the DF\n",
    "feature_imp_df = feature_imp_df.sort_values(by='Importance', ascending=False)\n",
    "\n",
    "# Add cumulative importance\n",
    "feature_imp_df['Cumulative Importance'] = feature_imp_df['Importance'].cumsum() / feature_imp_df['Importance'].sum()\n",
    "feature_imp_df['Percent Contribution'] = feature_imp_df['Cumulative Importance'] * 100\n",
    "\n",
    "top_n = 10\n",
    "print(f'Impact of top-{top_n} features to the Base Price: ${y_intercept:,.2f} (y-intercept)')\n",
    "feature_imp_df_styled = my_utils.df_style_floats(feature_imp_df[['Feature', 'Coefficient', 'Percent Contribution']]\n",
    "                                                 .set_index('Feature').rename(columns={'Coefficient':'Price Impact'}))\n",
    "\n",
    "# logging.getLogger().setLevel(logging.INFO)\n",
    "feature_imp_df_styled.head(top_n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d53d43a8-3dd0-4e35-ade4-31af059ee2bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Top features that explain X% of the variance\n",
    "top_var = .85\n",
    "top_features = feature_imp_df[feature_imp_df['Cumulative Importance'] <= top_var]\n",
    "\n",
    "# Plot feature importance\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.barh(top_features['Feature'], top_features['Coefficient'])\n",
    "plt.axvline(x=0, color=\"k\", linestyle=\"-\", label=f'Represents Base Price: ${y_intercept:,.2f}')\n",
    "\n",
    "plt.xlabel(f'Positive or Negative Impact of Individual Feature on Base Price', fontdict=my_utils.axes_fonts)\n",
    "plt.ylabel('Feature by Importance on Price', fontdict=my_utils.axes_fonts)\n",
    "\n",
    "plt.title(f'Top Features Explaining {top_var * 100:,.0f}% of Price Variance', fontdict=my_utils.title_fonts)\n",
    "plt.gca().invert_yaxis()\n",
    "plt.legend(loc='lower left')\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.savefig('images/feature_importance.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5be4f89-ab44-4b19-a620-c106ac250854",
   "metadata": {},
   "source": [
    "From the feature importance provided by the trained model, we can see the top features that capture upto 85% of the price variation, and individually represent the impact to price when there is a one-unit change in the corresponding feature. For example, our model is aware of the different `condition` values when predicting prices, with `condition_fair` negatively impacting price (-6,695.25) and `condition_new` positively impacting price."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f320b03-5540-4538-befa-0ee1d06c2128",
   "metadata": {},
   "source": [
    "#### Permutation Importance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85635a85-ee49-49f3-b397-51815906a930",
   "metadata": {},
   "source": [
    "Now that we have seen which features impact the price the most, let's calculate the permutation importance to measure the change in our model's performace when a feature value is randomly shuffled to see how much the model relies on that feature for its predictions. This will help us determine collinearity between features not captured by the model cofficients and evaluate the impact of changing the feature on the model performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42638326-2f76-4025-97ad-ec2d4a153cfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.inspection import permutation_importance\n",
    "\n",
    "p_imp = permutation_importance(best_model, X_test_scaled, y_test, n_repeats=10, random_state=random_state)\n",
    "p_imp_mean = p_imp.importances_mean\n",
    "p_imp_df = pd.DataFrame({'Feature': feature_names, 'Importance': p_imp_mean})\n",
    "p_imp_df.sort_values(by='Importance', ascending=False).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5602547a-ed23-4e5b-ba65-79daa5bfa92c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_permutation_importance(p_imp_result, features, p_imp_mean, ax):\n",
    "\n",
    "    perm_sorted_idx = p_imp_mean.argsort()\n",
    "\n",
    "    ax.boxplot(\n",
    "        p_imp_result.importances[perm_sorted_idx].T,\n",
    "        vert=False,\n",
    "        tick_labels=X.columns[perm_sorted_idx],\n",
    "    )\n",
    "    ax.axvline(x=0, color=\"k\", linestyle=\"--\")\n",
    "    return ax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc3a7955-6f62-4414-9f71-2d3e5423122e",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(8, 8))\n",
    "plot_permutation_importance(p_imp, feature_names, p_imp_mean, ax)\n",
    "ax.set_title(\"Permutation Importances on multicollinear features\", fontdict=my_utils.title_fonts)\n",
    "ax.set_xlabel(\"Decrease in accuracy score\")\n",
    "_ = ax.figure.tight_layout()\n",
    "\n",
    "plt.savefig('images/permutation_importance.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1b74088-1cc2-40e2-99b2-5b449a88828e",
   "metadata": {},
   "source": [
    "#### Comparing Feature / Permutation Importance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79857096-8995-49c2-b171-c08cc250cbc3",
   "metadata": {},
   "source": [
    "We see that there are slight differences between the Feature and Permuation Importance, so let's plot them side-by-side to better explain feature elimination to our customer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24f330fc-c69d-411e-bbf3-81b3a6753dd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(22, 10))\n",
    "\n",
    "fig.suptitle('Feature vs. Permutation Importance', fontdict=my_utils.title_fonts)\n",
    "# feature importance\n",
    "ax1.barh(feature_imp_df['Feature'], feature_imp_df['Coefficient'])\n",
    "ax1.axvline(x=0, color=\"k\", linestyle=\"-\", label=f'Base Price: ${y_intercept:,.2f}')\n",
    "\n",
    "# ax1.set_xlabel(f'Pos / Neg Impact of Individual Feature on Base Price', fontdict=my_utils.axes_fonts)\n",
    "# ax1.set_ylabel('Feature by Importance on Price', fontdict=my_utils.axes_fonts)\n",
    "\n",
    "ax1.set_title('')\n",
    "ax1.invert_yaxis()\n",
    "ax1.legend(loc='lower left')\n",
    "\n",
    "# perm imp\n",
    "plot_permutation_importance(p_imp, feature_names, p_imp_mean, ax2)\n",
    "ax2.set_title('')\n",
    "# ax2.set_xlabel(\"Decrease in accuracy score\", fontdict=my_utils.axes_fonts)\n",
    "\n",
    "_ = ax.figure.tight_layout()\n",
    "\n",
    "plt.savefig('images/feature_perm_importance.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e551a79d-2ac1-4cd9-adce-e62ccf3bd2b6",
   "metadata": {},
   "source": [
    "### Utility Functions: Segment Plotting\n",
    "\n",
    "Before analyzing the Feature and Permutation Importance information, let's define a couple of utility functions that facilitate plotting and tabulating the results in a standardized manner:\n",
    "\n",
    "* `fit_plot_segments`: Generate a scatter plot of Actual vs Predicted Prices given a standardized data structure containing criteria-based data segments. For each data segment:\n",
    "  * Create and scale train/test sets,\n",
    "  * Fit the training set and predict using the test set\n",
    "  * Returns a data structure to tabulate the metrics \n",
    "* `generate_segments_table`: Tabulates the results for each data segment from the above function\n",
    "\n",
    "These functions will be used for different segment analysis from this point forward and can be found in the `utils_practical_2.py` Python module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2179c590-8f80-4aac-ae40-41bae22a0506",
   "metadata": {},
   "outputs": [],
   "source": [
    "help(my_utils.fit_plot_segments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e337f2a6-720d-4e6f-82cc-6dca6152cb8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "help(my_utils.generate_segments_table)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12620a03-4910-4095-a6f7-4704b605e16a",
   "metadata": {},
   "source": [
    "Together, the graphs above show us the complete picture of the important features that determine price predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d79bd2a-b2be-4a90-ab37-21d703bcd1bb",
   "metadata": {},
   "source": [
    "## Feature Elimination & Optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6db3eada-fcbd-4f0f-97a9-2598916b68e9",
   "metadata": {},
   "source": [
    "Based on the above side-by-side comparison of the feature and permutation importance, we can see that a majority of the price is determeined by a relatively few features. Taking a look at the features that drive 85% of the price variance, the following features stand out, with different values impacting the price:\n",
    "\n",
    "* Odometer\n",
    "* Year\n",
    "* Condition\n",
    "* Transmission\n",
    "* Type\n",
    "* Drive\n",
    "* Fuel\n",
    "* Cylinders\n",
    "\n",
    "Based on the permutation importance, `year` pops to the top of the list. These results are intuitive and make sense from what we consider to be important considerations when pricing a used car.\n",
    "\n",
    "The other features can be eliminated as they are mostly colinear and detract from the accuracy score.\n",
    "\n",
    "Interestingly:\n",
    "\n",
    "* `title_status` was eliminated likely due to the vast majority of the cars having a 'clean' title\n",
    "* `size` is likely being over-shadowed by `transmission` and `cylinders` due to colinearity\n",
    "\n",
    "Let's run the best model again after removing the unnecessary features: `title_status`, `size`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b35f724-c647-4599-9d45-44f0d042a663",
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_cols = ['title_status', 'size']\n",
    "\n",
    "# drop_cols = ['title_status', 'cylinders','transmission']\n",
    "# drop_cols = ['title_status']\n",
    "data.drop(columns=drop_cols, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75322428-9dcf-4dbe-a279-74cc086b5a78",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d0ed6b5-f350-47d2-afb1-f5f4400dae28",
   "metadata": {},
   "source": [
    "Now we have narrowed down our focus to the above features. We recall from our initial data investigation that both `year` and `odometer` were identified as components of typical market segmentation of used cards. Let's analyze the distribution of these two features to check for outliers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c58c8438-4bbe-42f9-b9fe-c7fee9ef651b",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(10, 0.5))\n",
    "fig.suptitle('Odometer / Year highly skewed distribution before cutoffs', y=1.3)\n",
    "sns.boxenplot(ax=ax1, data=data, x='odometer')\n",
    "ax1.set_title('')\n",
    "\n",
    "sns.boxenplot(ax=ax2, data=data, x='year')\n",
    "ax2.set_title('')\n",
    "\n",
    "_ = ax.figure.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "900c964b-be0e-491c-b39f-caa6c0ed01e0",
   "metadata": {},
   "source": [
    "We see unrealistically skewed data that will affect our model training, so I will look for logical cutoffs to preserve data but also make it realistic from a pricing perspective:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b4b6cc5-f543-480b-b3aa-19d152b40e4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the odometer cutoff ranges for lo/hi milage\n",
    "odo_cutoff_lo = 0\n",
    "lo = data.query('odometer <= @odo_cutoff_lo')\n",
    "odo_cutoff_hi = 250000\n",
    "\n",
    "# Define year cutoff\n",
    "year_cutoff = 1980\n",
    "year_cutoff_lo = data.query('year <= @year_cutoff')\n",
    "temp = data.query('odometer > @odo_cutoff_lo and odometer <= @odo_cutoff_hi and year >= @year_cutoff')\n",
    "print('Odometer Cutoff Distribution: {:,d} ({:,.2f}%) <= lo cutoff({:,d} miles) <= {:,d} ({:,.2f}%) < hi cutoff ({:,d} miles) < {:,d} ({:,.2f}%)'\n",
    "      .format(lo.shape[0], ((lo.shape[0] / data.shape[0]) * 100), \n",
    "              odo_cutoff_lo,\n",
    "              temp.shape[0], ((temp.shape[0] / data.shape[0]) * 100),\n",
    "              odo_cutoff_hi, \n",
    "              data.shape[0] - temp.shape[0], ((1 - temp.shape[0] / data.shape[0]) * 100)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df469c27-856e-493b-9dc4-899f0bc9e83b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Year Cutoff Distribution: {:,d} ({:,.2f}%) <= cutoff({:d}) <= {:,d} ({:,.2f}%)'\n",
    "      .format(year_cutoff_lo.shape[0], ((year_cutoff_lo.shape[0] / data.shape[0]) * 100), \n",
    "              year_cutoff,\n",
    "              temp.shape[0], ((temp.shape[0] / data.shape[0]) * 100)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9db328ea-7722-41f5-b5a7-c7240db7ed27",
   "metadata": {},
   "outputs": [],
   "source": [
    "data=temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74daf13f-d6ab-45e4-8d15-e4efe16d88d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(10, 0.5))\n",
    "fig.suptitle('Odometer / Year distribution after cutoffs', y=1.3)\n",
    "sns.boxenplot(ax=ax1, data=data, x='odometer')\n",
    "ax1.set_title('')\n",
    "\n",
    "sns.boxenplot(ax=ax2, data=data, x='year')\n",
    "ax2.set_title('')\n",
    "\n",
    "_ = ax.figure.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c0e9859-0d92-4e7d-971c-b336e66ce42d",
   "metadata": {},
   "source": [
    "I decided to set the following cutoffs to get a more realistic inventory for re-trainig our best model:\n",
    "\n",
    "* Odometer: Restricted to milage between 0 and 250,000 miles\n",
    "  * Removed 232 low milage cars (0.19% of total) and 5,554 high-milage cars (4.53% of total)\n",
    "* Year: Removed 2,973 cars (2.42% of total) older than the 1980 model year\n",
    "\n",
    "Now, we're ready to re-train the model and analyze the results.\n",
    "\n",
    "NOTE: We will be using the segment reporting utility functions mentioned above: `fit_plot_segments` and `generate_segments_table`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b62fdb4e-7646-4e9f-8901-caae50147c3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data setup: Eliminated Features segment\n",
    "\n",
    "elim_segments = {\n",
    "    'Optimized Features': data\n",
    "}\n",
    "\n",
    "graph_x_label = 'Actual Prices'\n",
    "graph_y_label = 'Predicted Prices'\n",
    "elim_title = 'Optimized Features: Predictions vs Actuals'\n",
    "\n",
    "segments_dict = {\n",
    "    'Optimized Features': {\n",
    "        'seg_data': elim_segments,\n",
    "        'graph_title': elim_title,\n",
    "        'graph_x_label': graph_x_label,\n",
    "        'graph_y_label': graph_y_label,\n",
    "        'graph_filename': 'images/scatter-segments-opt-preds-v-test.png',\n",
    "        'seg_results': [],\n",
    "        'results_table_title': elim_title,\n",
    "        'results_table_filename': 'images/results_opt_segments.png'\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a683936c-413c-4af3-8bad-2f0df3793597",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot segments in single graph\n",
    "segment = 'Optimized Features'\n",
    "segments_dict[segment]['seg_results'] = my_utils.fit_plot_segments(segments_dict[segment], \n",
    "                                                                   segments_dict[segment]['seg_results'],\n",
    "                                                                   best_model, scaler, random_state,\n",
    "                                                                   segments_dict[segment]['graph_filename'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d93717e1-17e7-49f3-b7b8-0b089de5238d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate results table\n",
    "seg_df = my_utils.generate_segments_table(segment, \n",
    "                                          segments_dict[segment]['seg_results'], \n",
    "                                          png_filename=segments_dict[segment]['results_table_filename'])\n",
    "\n",
    "print(segments_dict[segment]['results_table_title'])\n",
    "seg_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91734194-a1b0-430e-8ee6-c07e4cf13ebb",
   "metadata": {},
   "source": [
    "We can see from the graph and the results table that our best Ridge Model really improved dramatically after optimization from 52.07% R2 score to 71.07%. In addition, the Average Price across all predictions is a more realistic \\\\$30,531."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad571a42-aab1-42a3-b4b1-aaf7386d25cd",
   "metadata": {},
   "source": [
    "## Model Applications: Segment Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afe5feed-836f-4c9b-bc92-b24f13c6f393",
   "metadata": {},
   "source": [
    "Now that the model has been re-trained, we can validate it using some of the findings from our data investigation where we found different ways to segment the pricing data. We will use our optimized model to predict prices for cars in these segments and see how our model performs. Please note that these new test segments includes cars from both the training and test sets, and we want to use this exercise as examples of interesting application of our Pricing Model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75af19a1-e68a-4482-bf9b-e41182aec629",
   "metadata": {},
   "source": [
    "### Market Defined Segments: Pricing Guidance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc7b5747-305d-4715-bc73-706a7dd7668c",
   "metadata": {},
   "source": [
    "We will now run the Pricing Model against a test set defined on the following market-based classification of our cars:\n",
    "\n",
    "* Budget: Model Year <= 2016 and Odometer >= 80,000\n",
    "* Entry: Model Year between 2016 and 2019, Odometer between 60,000 and 80,000\n",
    "* Mid: Model Year between 2020 and 2021, Odometer between 30,000 and 60,000\n",
    "* Premium: Model Year newer than 2021, Odometer less than  30,000\n",
    "\n",
    "This is typical of how used cars are categorized on the dealer lot. Let's see how we do! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6be429da-e80c-43a3-a203-6b9b2346aec6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Market Defintion of car segementations typical in the used car sales business\n",
    "\n",
    "mkt_segments = {\n",
    "    'Budget (YR <= 2016, Odo >= 80,000)': data.query('year <= 2016 and odometer >= 80000'),\n",
    "    'Entry (2016 >= YR <= 2019, 60,000 >= ODO <= 80,000)': data.query('(year>=2016 and year<=2019) and (odometer>=60000 and odometer<=80000)'),\n",
    "    'Mid (2019 >= YR <= 2021, 30,000 >= ODO <= 60,000)': data.query('(year>=2019 and year<=2021) and (odometer>=30000 and odometer<=60000)'),\n",
    "    'Premium (YR >= 2021, ODO >= 30,000)': data.query('(year>=2021 and year<=2024) and (odometer<=30000)'),\n",
    "}\n",
    "\n",
    "graph_x_label = 'Actual Prices'\n",
    "graph_y_label = 'Predicted Prices'\n",
    "mkt_title = 'Market-defined Segments: Predictions vs Actuals'\n",
    "\n",
    "segments_dict = {\n",
    "    'Market-defined Segments': {\n",
    "        'random_state': random_state,\n",
    "        'seg_data': mkt_segments,\n",
    "        'graph_title': mkt_title,\n",
    "        'graph_x_label': graph_x_label,\n",
    "        'graph_y_label': graph_y_label,\n",
    "        'graph_filename': 'images/scatter-segments-mkt-preds-v-test.png',\n",
    "        'seg_results': [],\n",
    "        'results_table_title': mkt_title,\n",
    "        'results_table_filename': 'images/results_mkt_segments.png'\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52012ff5-6066-4213-a3a0-f1bd9a6092bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot segments in single graph\n",
    "segment = 'Market-defined Segments'\n",
    "segments_dict[segment]['seg_results'] = my_utils.fit_plot_segments(segments_dict[segment], \n",
    "                                                                   segments_dict[segment]['seg_results'],\n",
    "                                                                   best_model, scaler, random_state,\n",
    "                                                                   segments_dict[segment]['graph_filename'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54811795-bfc2-4e00-98e7-71ab195d0cb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate results table\n",
    "seg_df = my_utils.generate_segments_table(segment, \n",
    "                                          segments_dict[segment]['seg_results'], \n",
    "                                          png_filename=segments_dict[segment]['results_table_filename'])\n",
    "\n",
    "print(segments_dict[segment]['results_table_title'])\n",
    "seg_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf324a90-36c0-467c-bc6a-b28e588436f6",
   "metadata": {},
   "source": [
    "As we can see, our model scores fairly well for the first three segments and is able to predict reasonable average prices for each segment. This is an example of using the Pricing Model in **Pricing Guidance** use cases for new inventory, where sales agents can input the vehicle features into the model to get suggested prices."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63688f99-b56a-4e4e-affc-3513f8be30e2",
   "metadata": {},
   "source": [
    "### Price-based Segments: Sale Price Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bb027ca-1315-472f-a450-b9c554c4763b",
   "metadata": {},
   "source": [
    "Next, we define a test set based on the following price-based classification of the cars:\n",
    "\n",
    "* Low-priced cars: Price < 5,000\n",
    "* Mid-priced cars: Prices between 5,000 and 50,000\n",
    "* High-priced cars: Price above 50,000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc53da97-2848-4083-b320-b20b9cff6bb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define segments\n",
    "mid_boundary = 5000\n",
    "high_boundary = 50000\n",
    "price_segments = {\n",
    "    'Low-priced (Price < 5,000)': data.query('price > 0 and price < @mid_boundary'),\n",
    "    'Mid-priced (5,000 >= Price < 50,000)': data.query('price >= @mid_boundary and price < @high_boundary'),\n",
    "    'High-priced (Price >= 50,000)': data.query('price >= @high_boundary'),\n",
    "}\n",
    "\n",
    "price_title = 'Price-based Segments: Predictions vs Actuals'\n",
    "\n",
    "segments_dict = {\n",
    "    'Price-based Segments': {\n",
    "        'random_state': random_state,\n",
    "        'seg_data': price_segments,\n",
    "        'graph_title': price_title,\n",
    "        'graph_x_label': graph_x_label,\n",
    "        'graph_y_label': graph_y_label,\n",
    "        'graph_filename': 'images/scatter-segments-price-preds-v-test.png',\n",
    "        'seg_results': [],\n",
    "        'results_table_title': price_title,\n",
    "        'results_table_filename': 'images/results_price_segments.png'\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d41b108-a4de-47d7-a314-ea7456288852",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot segments in single graph\n",
    "segment = 'Price-based Segments'\n",
    "segments_dict[segment]['seg_results'] = my_utils.fit_plot_segments(segments_dict[segment], \n",
    "                                                                   segments_dict[segment]['seg_results'],\n",
    "                                                                   best_model, scaler, random_state,\n",
    "                                                                   segments_dict[segment]['graph_filename'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af702fe0-6bb7-41ba-9486-1cb4ea33de62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate results table\n",
    "seg_df = my_utils.generate_segments_table(segment, \n",
    "                                          segments_dict[segment]['seg_results'], \n",
    "                                          png_filename=segments_dict[segment]['results_table_filename'])\n",
    "\n",
    "print(segments_dict[segment]['results_table_title'])\n",
    "seg_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebb3ff31-6aa7-4660-8e96-262f03535bda",
   "metadata": {},
   "source": [
    "We see that our model performs even better (73.98%) for the mid-priced range than what we have seen so far. For low- and hi-priced cars, there is likely insufficient data to model these cases. This gives us confidence that we can use this model to build a model to classify newly acquired inventory into the market-based segments from the previous Pricing Guidance example."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb95a2aa-13fb-4206-8e63-e3a3a2143c3f",
   "metadata": {},
   "source": [
    "### Validation Dataset: Defining the Null Data Segment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f0fc64e-8d1c-4d3b-b1d2-a064ef3f4648",
   "metadata": {},
   "source": [
    "If you go back and see how we cleaned the initial data set, we used `dropna()` to remove all nulls from the following columns before modeling to get a clean, high-quality dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bed5b3f-571b-4d43-9a6f-69fa41695a53",
   "metadata": {},
   "outputs": [],
   "source": [
    "dropna_cols"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97f71bf6-d9dc-49a4-8456-d8988384798a",
   "metadata": {},
   "source": [
    "At that time we had saved a copy of the rows that had atleast one column with a null value but included prices - we can treat this as a *validation dataset* to use our best model to predict prices. This is a good benchmark to see how it would do forecasting pricing for new autos that didn't have all the car attributes defined!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5eb5b6b9-97ea-4e17-8ff4-1afa60763aa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the data segement for rows with prices, but atleast one null-value column\n",
    "nulls_segment = {\n",
    "    'Data with Nulls': data_with_nulls\n",
    "}\n",
    "\n",
    "Null_title = 'Null Data Segment: Predictions vs Actuals'\n",
    "\n",
    "segments_dict = {\n",
    "    'Null Data Segments': {\n",
    "        'seg_data': nulls_segment,\n",
    "        'graph_title': Null_title,\n",
    "        'graph_x_label': graph_x_label,\n",
    "        'graph_y_label': graph_y_label,\n",
    "        'graph_filename': 'images/scatter-segments-nulls-preds-v-test.png',\n",
    "        'seg_results': [],\n",
    "        'results_table_title': Null_title,\n",
    "        'results_table_filename': 'images/results_nulls_segments.png'\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcdb1662-51d0-45d3-85a4-9298d28d4672",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot segments in single graph\n",
    "segment = 'Null Data Segments'\n",
    "segments_dict[segment]['seg_results'] = my_utils.fit_plot_segments(segments_dict[segment], \n",
    "                                                                   segments_dict[segment]['seg_results'],\n",
    "                                                                   best_model, scaler, random_state,\n",
    "                                                                   segments_dict[segment]['graph_filename'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d644e91-8fd7-4ec8-b31c-e22edd2856b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate results table\n",
    "seg_df = my_utils.generate_segments_table(segment, \n",
    "                                          segments_dict[segment]['seg_results'], \n",
    "                                          png_filename=segments_dict[segment]['results_table_filename'])\n",
    "\n",
    "print(segments_dict[segment]['results_table_title'])\n",
    "seg_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4e4bbc2-3105-4f2b-a15a-6d172d7ff411",
   "metadata": {},
   "source": [
    "# Deployment\n",
    "\n",
    "> **Prompt**: Now that we've settled on our models and findings, it is time to deliver the information to the client.\n",
    "> You should organize your work as a basic report that details your primary findings.  Keep in mind that your\n",
    "> audience is a group of used car dealers interested in fine tuning their inventory.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c2b3a35-535e-4e9c-a12a-4cf96b8ebc5f",
   "metadata": {},
   "source": [
    "Since we are not deploying this model in a production environment yet, we will provide a Plain Language Report to the customer that summarizes our results using the CRISP-DM format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48db215e-42c9-4f67-844f-b3a349d9206b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
